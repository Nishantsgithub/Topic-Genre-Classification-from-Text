{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:00:18.625532Z",
     "start_time": "2020-04-02T15:00:17.377733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from time import localtime, strftime\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "import zipfile\n",
    "import gc\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw texts into training and development data\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the CSV files using the pandas library \n",
    "\n",
    "Each data frame has two columns - 'Class' and 'News Articles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.748484Z",
     "start_time": "2020-04-02T14:26:39.727404Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"D:/Data Analytics/Natural Language Processing/Assignment/data_topic/train.csv\", names=['Class', 'News Articles'])\n",
    "\n",
    "\n",
    "dev_df = pd.read_csv(\"D:/Data Analytics/Natural Language Processing/Assignment/data_topic/dev.csv\", names=['Class', 'News Articles'])\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(\"D:/Data Analytics/Natural Language Processing/Assignment/data_topic/test.csv\", names=['Class', 'News Articles'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>News Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Venezuelans turned out early\\and in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - South Korean police used water canno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Thousands of Palestinian\\prisoners i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Sporadic gunfire and shelling took place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - Dozens of Rwandan soldiers flew into Suda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                      News Articles\n",
       "0      1  Reuters - Venezuelans turned out early\\and in ...\n",
       "1      1  Reuters - South Korean police used water canno...\n",
       "2      1  Reuters - Thousands of Palestinian\\prisoners i...\n",
       "3      1  AFP - Sporadic gunfire and shelling took place...\n",
       "4      1  AP - Dozens of Rwandan soldiers flew into Suda..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)\n",
    "\n",
    "#Checking the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the data frames to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.753874Z",
     "start_time": "2020-04-02T14:26:39.749647Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text = list(train_df['News Articles'])\n",
    "dev_text = list(dev_df['News Articles'])\n",
    "test_text = list(test_df['News Articles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input representations\n",
    "\n",
    "\n",
    "To train your Feedforward network, you first need to obtain input representations given a vocabulary. One-hot encoding requires large memory capacity. Therefore, we will instead represent documents as lists of vocabulary indices (each word corresponds to a vocabulary index). \n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of words. You should: \n",
    "- tokenise all texts into a list of unigrams (tip: you can re-use the functions from Assignment 1) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- remove unigrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of the top-N most frequent unigrams in the entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:40.851926Z",
     "start_time": "2020-04-02T14:26:40.847500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i', 'if',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n",
    "              'but', 'not', 'there', 'no', 'does', 'not', 'so', 've', 'their',\n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(text, ngram_range=(1,3),token_pattern = r'\\b[A-Za-z][A-Za-z]+\\b', stop_words=[], vocab=set()):\n",
    "    \n",
    "    # first extract all unigrams by tokenising\n",
    "    tokens = re.findall(token_pattern, text.lower())\n",
    "    \n",
    "    # Remove stop words from the tokens list\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # Initialize an empty list to store the n-grams\n",
    "    ngrams = []\n",
    "    \n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            ngram = ' '.join(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "            \n",
    "     # Only keep the n-grams that appear in the vocabulary\n",
    "    if len(vocab)>0:\n",
    "        ngrams = [w for w in ngrams if w in vocab]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract_ngrams function\n",
    "\n",
    "The function initially tokenizes the input text using the regular expression pattern specified by token_pattern to create a list of lowercase terms. The resulting list of tokens is then purged of any stop words.\n",
    "\n",
    "After that, it creates an empty list to hold the n-grams and iterates through the set of n-gram sizes defined by ngram_range, and for each size, it iterates over every feasible place to link the tokens into an n-gram of that size.\n",
    "\n",
    "The function then outputs a list of n-grams and filters the list of n-grams to only include those that are part of the vocabulary set if the vocab input is not null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reuters', 'venezuelans', 'turned', 'out', 'early', 'large', 'numbers', 'sunday', 'vote', 'historic', 'referendum', 'either', 'remove', 'left', 'wing', 'president', 'hugo', 'chavez', 'office', 'give', 'him', 'new', 'mandate', 'govern', 'next', 'two', 'years', 'reuters venezuelans', 'venezuelans turned', 'turned out', 'out early', 'early large', 'large numbers', 'numbers sunday', 'sunday vote', 'vote historic', 'historic referendum', 'referendum either', 'either remove', 'remove left', 'left wing', 'wing president', 'president hugo', 'hugo chavez', 'chavez office', 'office give', 'give him', 'him new', 'new mandate', 'mandate govern', 'govern next', 'next two', 'two years', 'reuters venezuelans turned', 'venezuelans turned out', 'turned out early', 'out early large', 'early large numbers', 'large numbers sunday', 'numbers sunday vote', 'sunday vote historic', 'vote historic referendum', 'historic referendum either', 'referendum either remove', 'either remove left', 'remove left wing', 'left wing president', 'wing president hugo', 'president hugo chavez', 'hugo chavez office', 'chavez office give', 'office give him', 'give him new', 'him new mandate', 'new mandate govern', 'mandate govern next', 'govern next two', 'next two years']\n"
     ]
    }
   ],
   "source": [
    "#checking the function for first row values in train text \n",
    "x1=train_text\n",
    "ngrams = extract_ngrams(x1[0], ngram_range=(1,3), stop_words=stop_words)\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:42.563876Z",
     "start_time": "2020-04-02T14:26:42.557967Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "              min_df=0, keep_topN=0, stop_words=[]):\n",
    "   \n",
    "    # compile the regular expression pattern for tokenization\n",
    "    tokens = re.compile(token_pattern)\n",
    "   \n",
    "    # initialize counters for document frequency and n-gram counts\n",
    "    df = Counter()\n",
    "    ngram_counts = Counter()\n",
    "    \n",
    "    # iterate through each raw text in X_raw\n",
    "    for x in X_raw:\n",
    "        \n",
    "        # extract n-grams from the current text using the given n-gram range,\n",
    "        # token pattern, and stop words list\n",
    "        x_ngram = extract_ngrams(x, ngram_range=ngram_range, token_pattern=token_pattern, stop_words=stop_words)\n",
    "        \n",
    "        # update the document frequency counter with the unique n-grams in the current text\n",
    "        df.update(list(set(x_ngram)))\n",
    "        \n",
    "        # update the n-gram counts counter with all n-grams in the current text\n",
    "        ngram_counts.update(x_ngram)\n",
    "\n",
    "    # obtain a set of all n-grams that meet the minimum document frequency threshold\n",
    "    vocab = set([words for words in df if df[words]>=min_df])\n",
    "   \n",
    "    # if a positive value for keep_topN is specified,\n",
    "    if keep_topN>0:\n",
    "        vocab = set([words[0] for words in ngram_counts.most_common(keep_topN) if words[0] in vocab])\n",
    "   \n",
    "   \n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_vocab function\n",
    "\n",
    "The get_vocab function takes in raw text data and returns a set of n-grams (words or phrases) that meet certain criteria such as minimum document frequency and maximum number of n-grams to keep.\n",
    "\n",
    "\n",
    "\n",
    "The re.compile function is used to first compile the regular expression pattern for tokenization. Then, it initialises two counters: ngram_counts, which keeps track of the raw frequency of n-grams, and df, which tracks the frequency of n-grams in documents.\n",
    "\n",
    "Following that, the function loops through each raw text in the input list X_raw. The extract_ngrams function is called for each text in order to extract the n-grams with the given parameters, updating the df and ngram_counts counters in the process.\n",
    "\n",
    "The set() and if statements are used by the function to obtain a set of all n-grams that satisfy the minimum document frequency threshold after iterating through all texts.\n",
    "The function also includes the df and ngram_counts counters in its final output, which includes the set of n-grams in the vocabulary.\n",
    "\n",
    "The function also outputs the Raw frequencies as well as the count of the n-grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:43.577997Z",
     "start_time": "2020-04-02T14:26:43.478950Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "Vocab: \n",
      " ['drop', 'challenges', 'jason', 'surging', 'idea', 'elbow', 'shoulder', 'toyota', 'contract', 'latest', 'women', 'east', 'areas', 'caracas', 'star', 'day', 'say', 'according', 'all', 'lt', 'missed', 'protesters', 'son', 'debut', 'black', 'industry', 'recall', 'order', 'withdraw', 'again', 'issued', 'vice', 'one', 'jean', 'including', 'sunday', 'trees', 'quote', 'edwards', 'managers', 'successful', 'saying', 'struck', 'blue', 'negotiations', 'issues', 'al', 'improve', 'return', 'afghan']\n",
      "\n",
      " Raw frequencies of n-grams: \n",
      " [('reuters', 631), ('said', 432), ('tuesday', 413), ('wednesday', 344), ('new', 325), ('after', 295), ('ap', 275), ('athens', 245), ('monday', 221), ('first', 210)]\n",
      "\n",
      " Counts of each ngram in vocab \n",
      " [('reuters', 694), ('said', 440), ('tuesday', 415), ('new', 365), ('wednesday', 346), ('after', 304), ('athens', 293), ('ap', 276), ('monday', 221), ('first', 219)]\n"
     ]
    }
   ],
   "source": [
    "#Checking the vocab function for 50 values and creating the vocabulary for train ,test and development data \n",
    "\n",
    "vocab,df,ngram_counts = get_vocab(train_text, ngram_range=(1,1), min_df=2, stop_words=stop_words, keep_topN=2000)\n",
    "vocab_dev, df_dev, ngram_counts_dev = get_vocab(dev_text, ngram_range=(1,1), min_df =2, stop_words=stop_words, keep_topN=2000)\n",
    "vocab_test, df_test, ngram_counts_test = get_vocab(test_text, ngram_range=(1,1), min_df=2, stop_words=stop_words, keep_topN=2000)\n",
    "print(len(vocab))\n",
    "print(\"Vocab: \\n\", list(vocab)[:50])\n",
    "print(\"\\n Raw frequencies of n-grams: \\n\", df.most_common()[:10])\n",
    "print(\"\\n Counts of each ngram in vocab \\n\", ngram_counts.most_common()[:10])\n",
    "\n",
    "#Here the min_df is kept to 2 as an n-gram must appear in at least 2 documents in the training data to be included in the vocabulary.\n",
    "\n",
    "# Also note that the keep_topN is set to 2000 , to limit the words in vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and word -> vocabulary id dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:44.069661Z",
     "start_time": "2020-04-02T14:26:44.065058Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drop': 0,\n",
       " 'challenges': 1,\n",
       " 'jason': 2,\n",
       " 'surging': 3,\n",
       " 'idea': 4,\n",
       " 'elbow': 5,\n",
       " 'shoulder': 6,\n",
       " 'toyota': 7,\n",
       " 'contract': 8,\n",
       " 'latest': 9,\n",
       " 'women': 10,\n",
       " 'east': 11,\n",
       " 'areas': 12,\n",
       " 'caracas': 13,\n",
       " 'star': 14,\n",
       " 'day': 15,\n",
       " 'say': 16,\n",
       " 'according': 17,\n",
       " 'all': 18,\n",
       " 'lt': 19,\n",
       " 'missed': 20,\n",
       " 'protesters': 21,\n",
       " 'son': 22,\n",
       " 'debut': 23,\n",
       " 'black': 24,\n",
       " 'industry': 25,\n",
       " 'recall': 26,\n",
       " 'order': 27,\n",
       " 'withdraw': 28,\n",
       " 'again': 29,\n",
       " 'issued': 30,\n",
       " 'vice': 31,\n",
       " 'one': 32,\n",
       " 'jean': 33,\n",
       " 'including': 34,\n",
       " 'sunday': 35,\n",
       " 'trees': 36,\n",
       " 'quote': 37,\n",
       " 'edwards': 38,\n",
       " 'managers': 39,\n",
       " 'successful': 40,\n",
       " 'saying': 41,\n",
       " 'struck': 42,\n",
       " 'blue': 43,\n",
       " 'negotiations': 44,\n",
       " 'issues': 45,\n",
       " 'al': 46,\n",
       " 'improve': 47,\n",
       " 'return': 48,\n",
       " 'afghan': 49,\n",
       " 'climbed': 50,\n",
       " 'closely': 51,\n",
       " 'island': 52,\n",
       " 'third': 53,\n",
       " 'alexander': 54,\n",
       " 'darfur': 55,\n",
       " 'hot': 56,\n",
       " 'plotting': 57,\n",
       " 'prime': 58,\n",
       " 'join': 59,\n",
       " 'trade': 60,\n",
       " 'sending': 61,\n",
       " 'free': 62,\n",
       " 'mdt': 63,\n",
       " 'leader': 64,\n",
       " 'wing': 65,\n",
       " 'brendan': 66,\n",
       " 'family': 67,\n",
       " 'man': 68,\n",
       " 'remote': 69,\n",
       " 'path': 70,\n",
       " 'profile': 71,\n",
       " 'name': 72,\n",
       " 'delegates': 73,\n",
       " 'enter': 74,\n",
       " 'lumpur': 75,\n",
       " 'ii': 76,\n",
       " 'above': 77,\n",
       " 'days': 78,\n",
       " 'consider': 79,\n",
       " 'wave': 80,\n",
       " 'kathmandu': 81,\n",
       " 'talk': 82,\n",
       " 'militant': 83,\n",
       " 'weapons': 84,\n",
       " 'woman': 85,\n",
       " 'ago': 86,\n",
       " 'point': 87,\n",
       " 'wants': 88,\n",
       " 'tim': 89,\n",
       " 'restaurants': 90,\n",
       " 'done': 91,\n",
       " 'adam': 92,\n",
       " 'opec': 93,\n",
       " 'dropped': 94,\n",
       " 'republican': 95,\n",
       " 'bush': 96,\n",
       " 'prison': 97,\n",
       " 'sadr': 98,\n",
       " 'gt': 99,\n",
       " 'tiger': 100,\n",
       " 'linked': 101,\n",
       " 'expectations': 102,\n",
       " 'stop': 103,\n",
       " 'had': 104,\n",
       " 'olympic': 105,\n",
       " 'call': 106,\n",
       " 'laws': 107,\n",
       " 'kuala': 108,\n",
       " 'series': 109,\n",
       " 'immediately': 110,\n",
       " 'offers': 111,\n",
       " 'villa': 112,\n",
       " 'troubled': 113,\n",
       " 'allowed': 114,\n",
       " 'convicted': 115,\n",
       " 'lack': 116,\n",
       " 'former': 117,\n",
       " 'key': 118,\n",
       " 'canada': 119,\n",
       " 'large': 120,\n",
       " 'louis': 121,\n",
       " 'set': 122,\n",
       " 'girl': 123,\n",
       " 'businesses': 124,\n",
       " 'improvements': 125,\n",
       " 'murder': 126,\n",
       " 'network': 127,\n",
       " 'ninth': 128,\n",
       " 'october': 129,\n",
       " 'having': 130,\n",
       " 'nuclear': 131,\n",
       " 'lead': 132,\n",
       " 'doubled': 133,\n",
       " 'ministry': 134,\n",
       " 'ryder': 135,\n",
       " 'single': 136,\n",
       " 'toward': 137,\n",
       " 'profit': 138,\n",
       " 'australia': 139,\n",
       " 'breaststroke': 140,\n",
       " 'raised': 141,\n",
       " 'wild': 142,\n",
       " 'held': 143,\n",
       " 'paperwork': 144,\n",
       " 'per': 145,\n",
       " 'blow': 146,\n",
       " 'negotiate': 147,\n",
       " 'emergency': 148,\n",
       " 'euros': 149,\n",
       " 'home': 150,\n",
       " 'criminal': 151,\n",
       " 'roddick': 152,\n",
       " 'system': 153,\n",
       " 'americans': 154,\n",
       " 'quarterback': 155,\n",
       " 'kmart': 156,\n",
       " 'portugal': 157,\n",
       " 'programme': 158,\n",
       " 'declared': 159,\n",
       " 'leg': 160,\n",
       " 'crowded': 161,\n",
       " 'most': 162,\n",
       " 'doping': 163,\n",
       " 'brian': 164,\n",
       " 'medal': 165,\n",
       " 'find': 166,\n",
       " 'sprinters': 167,\n",
       " 'yankees': 168,\n",
       " 'selling': 169,\n",
       " 'chance': 170,\n",
       " 'press': 171,\n",
       " 'expected': 172,\n",
       " 'international': 173,\n",
       " 'food': 174,\n",
       " 'tie': 175,\n",
       " 'fourth': 176,\n",
       " 'sentiment': 177,\n",
       " 'taking': 178,\n",
       " 'board': 179,\n",
       " 'without': 180,\n",
       " 'working': 181,\n",
       " 'violation': 182,\n",
       " 'par': 183,\n",
       " 'meet': 184,\n",
       " 'first': 185,\n",
       " 'retailer': 186,\n",
       " 'look': 187,\n",
       " 'falling': 188,\n",
       " 'swept': 189,\n",
       " 'ease': 190,\n",
       " 'decade': 191,\n",
       " 'rise': 192,\n",
       " 'anti': 193,\n",
       " 'road': 194,\n",
       " 'finals': 195,\n",
       " 'mutual': 196,\n",
       " 'becoming': 197,\n",
       " 'where': 198,\n",
       " 'told': 199,\n",
       " 'green': 200,\n",
       " 'time': 201,\n",
       " 'motors': 202,\n",
       " 'charge': 203,\n",
       " 'always': 204,\n",
       " 'sweep': 205,\n",
       " 'foreign': 206,\n",
       " 'singles': 207,\n",
       " 'controversial': 208,\n",
       " 'wednesday': 209,\n",
       " 'months': 210,\n",
       " 'appealed': 211,\n",
       " 'acquisition': 212,\n",
       " 'pontiff': 213,\n",
       " 'area': 214,\n",
       " 'plc': 215,\n",
       " 'embassy': 216,\n",
       " 'radioactive': 217,\n",
       " 'higher': 218,\n",
       " 'quarterly': 219,\n",
       " 'outside': 220,\n",
       " 'touted': 221,\n",
       " 'vote': 222,\n",
       " 'medallist': 223,\n",
       " 'doubts': 224,\n",
       " 'television': 225,\n",
       " 'dimarco': 226,\n",
       " 'opening': 227,\n",
       " 'well': 228,\n",
       " 'sex': 229,\n",
       " 'refinancings': 230,\n",
       " 'dream': 231,\n",
       " 'serving': 232,\n",
       " 'more': 233,\n",
       " 'market': 234,\n",
       " 'slam': 235,\n",
       " 'falconio': 236,\n",
       " 'cars': 237,\n",
       " 'upi': 238,\n",
       " 'katerina': 239,\n",
       " 'drugs': 240,\n",
       " 'slow': 241,\n",
       " 'billiton': 242,\n",
       " 'jr': 243,\n",
       " 'colorado': 244,\n",
       " 'led': 245,\n",
       " 'mortgage': 246,\n",
       " 'charges': 247,\n",
       " 'showed': 248,\n",
       " 'while': 249,\n",
       " 'pile': 250,\n",
       " 'try': 251,\n",
       " 'khartoum': 252,\n",
       " 'ipo': 253,\n",
       " 'jamaica': 254,\n",
       " 'fellow': 255,\n",
       " 'cash': 256,\n",
       " 'hit': 257,\n",
       " 'term': 258,\n",
       " 'company': 259,\n",
       " 'holiday': 260,\n",
       " 'games': 261,\n",
       " 'finally': 262,\n",
       " 'seen': 263,\n",
       " 'million': 264,\n",
       " 'fast': 265,\n",
       " 'august': 266,\n",
       " 'table': 267,\n",
       " 'charley': 268,\n",
       " 'bids': 269,\n",
       " 'streak': 270,\n",
       " 'backed': 271,\n",
       " 'continued': 272,\n",
       " 'venus': 273,\n",
       " 'among': 274,\n",
       " 'products': 275,\n",
       " 'steve': 276,\n",
       " 'watched': 277,\n",
       " 'long': 278,\n",
       " 'judge': 279,\n",
       " 'appeal': 280,\n",
       " 'picture': 281,\n",
       " 'consecutive': 282,\n",
       " 'lives': 283,\n",
       " 'bloomberg': 284,\n",
       " 'second': 285,\n",
       " 'professional': 286,\n",
       " 'costs': 287,\n",
       " 'such': 288,\n",
       " 'tomorrow': 289,\n",
       " 'interview': 290,\n",
       " 'involving': 291,\n",
       " 'maria': 292,\n",
       " 'rico': 293,\n",
       " 'dallas': 294,\n",
       " 'signed': 295,\n",
       " 'vegas': 296,\n",
       " 'georgian': 297,\n",
       " 'massive': 298,\n",
       " 'showing': 299,\n",
       " 'thursday': 300,\n",
       " 'line': 301,\n",
       " 'claims': 302,\n",
       " 'being': 303,\n",
       " 'leaders': 304,\n",
       " 'flew': 305,\n",
       " 'strength': 306,\n",
       " 'patriots': 307,\n",
       " 'ite': 308,\n",
       " 'battled': 309,\n",
       " 'launched': 310,\n",
       " 'street': 311,\n",
       " 'ailing': 312,\n",
       " 'nelson': 313,\n",
       " 'classic': 314,\n",
       " 'delegation': 315,\n",
       " 'cardinals': 316,\n",
       " 'beginning': 317,\n",
       " 'reached': 318,\n",
       " 'staff': 319,\n",
       " 'running': 320,\n",
       " 'florida': 321,\n",
       " 'fischer': 322,\n",
       " 'rwandan': 323,\n",
       " 'period': 324,\n",
       " 'wife': 325,\n",
       " 'religious': 326,\n",
       " 'settlements': 327,\n",
       " 'creating': 328,\n",
       " 'minutes': 329,\n",
       " 'pre': 330,\n",
       " 'event': 331,\n",
       " 'lay': 332,\n",
       " 'heavy': 333,\n",
       " 'your': 334,\n",
       " 'post': 335,\n",
       " 'quickly': 336,\n",
       " 'greece': 337,\n",
       " 'akron': 338,\n",
       " 'revealed': 339,\n",
       " 'magazine': 340,\n",
       " 'customers': 341,\n",
       " 'past': 342,\n",
       " 'car': 343,\n",
       " 'low': 344,\n",
       " 'amateur': 345,\n",
       " 'slashed': 346,\n",
       " 'november': 347,\n",
       " 'rules': 348,\n",
       " 'like': 349,\n",
       " 'agreement': 350,\n",
       " 'house': 351,\n",
       " 'offering': 352,\n",
       " 'deal': 353,\n",
       " 'state': 354,\n",
       " 'supply': 355,\n",
       " 'ag': 356,\n",
       " 'injured': 357,\n",
       " 'drew': 358,\n",
       " 'seven': 359,\n",
       " 'stunning': 360,\n",
       " 'same': 361,\n",
       " 'getting': 362,\n",
       " 'knew': 363,\n",
       " 'violence': 364,\n",
       " 'costco': 365,\n",
       " 'chain': 366,\n",
       " 'gained': 367,\n",
       " 'coast': 368,\n",
       " 'dismissed': 369,\n",
       " 'straits': 370,\n",
       " 'now': 371,\n",
       " 'waiting': 372,\n",
       " 'indians': 373,\n",
       " 'lawyers': 374,\n",
       " 'teams': 375,\n",
       " 'nestle': 376,\n",
       " 'looks': 377,\n",
       " 'surgery': 378,\n",
       " 'positive': 379,\n",
       " 'gary': 380,\n",
       " 'slower': 381,\n",
       " 'toll': 382,\n",
       " 'size': 383,\n",
       " 'securities': 384,\n",
       " 'face': 385,\n",
       " 'miracle': 386,\n",
       " 'army': 387,\n",
       " 'popular': 388,\n",
       " 'prices': 389,\n",
       " 'online': 390,\n",
       " 'suffered': 391,\n",
       " 'sydney': 392,\n",
       " 'rather': 393,\n",
       " 'better': 394,\n",
       " 'euro': 395,\n",
       " 'beating': 396,\n",
       " 'jail': 397,\n",
       " 'details': 398,\n",
       " 'futures': 399,\n",
       " 'almost': 400,\n",
       " 'monday': 401,\n",
       " 'front': 402,\n",
       " 'delivery': 403,\n",
       " 'pinch': 404,\n",
       " 'facing': 405,\n",
       " 'future': 406,\n",
       " 'phone': 407,\n",
       " 'becomes': 408,\n",
       " 'ap': 409,\n",
       " 'helped': 410,\n",
       " 'office': 411,\n",
       " 'citing': 412,\n",
       " 'senate': 413,\n",
       " 'disciplinary': 414,\n",
       " 'liverpool': 415,\n",
       " 'preseason': 416,\n",
       " 'governor': 417,\n",
       " 'opponent': 418,\n",
       " 'fear': 419,\n",
       " 'executives': 420,\n",
       " 'clashes': 421,\n",
       " 'something': 422,\n",
       " 'double': 423,\n",
       " 'peter': 424,\n",
       " 'inventory': 425,\n",
       " 'armed': 426,\n",
       " 'surge': 427,\n",
       " 'hitting': 428,\n",
       " 'strong': 429,\n",
       " 'before': 430,\n",
       " 'claimed': 431,\n",
       " 'statement': 432,\n",
       " 'militants': 433,\n",
       " 'administration': 434,\n",
       " 'fund': 435,\n",
       " 'baseman': 436,\n",
       " 'hunger': 437,\n",
       " 'western': 438,\n",
       " 'although': 439,\n",
       " 'rangers': 440,\n",
       " 'hospitals': 441,\n",
       " 'aug': 442,\n",
       " 'goal': 443,\n",
       " 'suspended': 444,\n",
       " 'operating': 445,\n",
       " 'proposal': 446,\n",
       " 'up': 447,\n",
       " 'investors': 448,\n",
       " 'own': 449,\n",
       " 'holiest': 450,\n",
       " 'amid': 451,\n",
       " 'butterfly': 452,\n",
       " 'thumb': 453,\n",
       " 'battles': 454,\n",
       " 'calm': 455,\n",
       " 'reported': 456,\n",
       " 'buried': 457,\n",
       " 'violations': 458,\n",
       " 'alleged': 459,\n",
       " 'manage': 460,\n",
       " 'ancient': 461,\n",
       " 'down': 462,\n",
       " 'sec': 463,\n",
       " 'teammate': 464,\n",
       " 'crash': 465,\n",
       " 'jobs': 466,\n",
       " 'game': 467,\n",
       " 'arrived': 468,\n",
       " 'congolese': 469,\n",
       " 'evidence': 470,\n",
       " 'hold': 471,\n",
       " 'facilities': 472,\n",
       " 'light': 473,\n",
       " 'policy': 474,\n",
       " 'olympics': 475,\n",
       " 'pittsburgh': 476,\n",
       " 'extra': 477,\n",
       " 'rights': 478,\n",
       " 'freddie': 479,\n",
       " 'chicago': 480,\n",
       " 'looking': 481,\n",
       " 'technology': 482,\n",
       " 'town': 483,\n",
       " 'hoogenband': 484,\n",
       " 'forward': 485,\n",
       " 'himself': 486,\n",
       " 'georgia': 487,\n",
       " 'begin': 488,\n",
       " 'prisoners': 489,\n",
       " 'paul': 490,\n",
       " 'treasury': 491,\n",
       " 'johnson': 492,\n",
       " 'trial': 493,\n",
       " 'demand': 494,\n",
       " 'few': 495,\n",
       " 'prosecutors': 496,\n",
       " 'playing': 497,\n",
       " 'local': 498,\n",
       " 'spitz': 499,\n",
       " 'cup': 500,\n",
       " 'asked': 501,\n",
       " 'urged': 502,\n",
       " 'bad': 503,\n",
       " 'los': 504,\n",
       " 'club': 505,\n",
       " 'japanese': 506,\n",
       " 'easy': 507,\n",
       " 'pentagon': 508,\n",
       " 'bring': 509,\n",
       " 'drive': 510,\n",
       " 'starting': 511,\n",
       " 'chess': 512,\n",
       " 'knocked': 513,\n",
       " 'evening': 514,\n",
       " 'ban': 515,\n",
       " 'force': 516,\n",
       " 'moved': 517,\n",
       " 'hurt': 518,\n",
       " 'athens': 519,\n",
       " 'west': 520,\n",
       " 'spokesman': 521,\n",
       " 'globe': 522,\n",
       " 'advertising': 523,\n",
       " 'sites': 524,\n",
       " 'bid': 525,\n",
       " 'released': 526,\n",
       " 'baltimore': 527,\n",
       " 'renewed': 528,\n",
       " 'twins': 529,\n",
       " 'arbitration': 530,\n",
       " 'raise': 531,\n",
       " 'overboard': 532,\n",
       " 'quest': 533,\n",
       " 'newly': 534,\n",
       " 'complete': 535,\n",
       " 'would': 536,\n",
       " 'team': 537,\n",
       " 'japan': 538,\n",
       " 'bell': 539,\n",
       " 'management': 540,\n",
       " 'sales': 541,\n",
       " 'victims': 542,\n",
       " 'ex': 543,\n",
       " 'lawsuit': 544,\n",
       " 'twenty': 545,\n",
       " 'county': 546,\n",
       " 'flash': 547,\n",
       " 'minute': 548,\n",
       " 'historic': 549,\n",
       " 'data': 550,\n",
       " 'might': 551,\n",
       " 'arab': 552,\n",
       " 'chavez': 553,\n",
       " 'ibm': 554,\n",
       " 'service': 555,\n",
       " 'homes': 556,\n",
       " 'power': 557,\n",
       " 'center': 558,\n",
       " 'possible': 559,\n",
       " 'sixth': 560,\n",
       " 'keller': 561,\n",
       " 'democracy': 562,\n",
       " 'counting': 563,\n",
       " 'eight': 564,\n",
       " 'gov': 565,\n",
       " 'research': 566,\n",
       " 'insurer': 567,\n",
       " 'lower': 568,\n",
       " 'tomas': 569,\n",
       " 'jose': 570,\n",
       " 'place': 571,\n",
       " 'history': 572,\n",
       " 'across': 573,\n",
       " 'weekend': 574,\n",
       " 'shooting': 575,\n",
       " 'coach': 576,\n",
       " 'iran': 577,\n",
       " 'dead': 578,\n",
       " 'city': 579,\n",
       " 'nation': 580,\n",
       " 'winds': 581,\n",
       " 'royal': 582,\n",
       " 'catch': 583,\n",
       " 'earn': 584,\n",
       " 'shrugged': 585,\n",
       " 'threatening': 586,\n",
       " 'yukos': 587,\n",
       " 'yasser': 588,\n",
       " 'kong': 589,\n",
       " 'view': 590,\n",
       " 'hungarian': 591,\n",
       " 'ethnic': 592,\n",
       " 'inquiry': 593,\n",
       " 'analysts': 594,\n",
       " 'make': 595,\n",
       " 'amp': 596,\n",
       " 'missing': 597,\n",
       " 'reportedly': 598,\n",
       " 'offer': 599,\n",
       " 'dozens': 600,\n",
       " 'general': 601,\n",
       " 'prince': 602,\n",
       " 'oust': 603,\n",
       " 'lowest': 604,\n",
       " 'giants': 605,\n",
       " 'cycling': 606,\n",
       " 'promotion': 607,\n",
       " 'prosecutor': 608,\n",
       " 'defence': 609,\n",
       " 'walk': 610,\n",
       " 'opener': 611,\n",
       " 'lowe': 612,\n",
       " 'estimated': 613,\n",
       " 'security': 614,\n",
       " 'only': 615,\n",
       " 'offset': 616,\n",
       " 'died': 617,\n",
       " 'enough': 618,\n",
       " 'independence': 619,\n",
       " 'gave': 620,\n",
       " 'compete': 621,\n",
       " 'accused': 622,\n",
       " 'debt': 623,\n",
       " 'british': 624,\n",
       " 'corporate': 625,\n",
       " 'leftist': 626,\n",
       " 'hitter': 627,\n",
       " 'cbs': 628,\n",
       " 'auto': 629,\n",
       " 'leonard': 630,\n",
       " 'revolution': 631,\n",
       " 'quit': 632,\n",
       " 'against': 633,\n",
       " 'insurers': 634,\n",
       " 'tour': 635,\n",
       " 'search': 636,\n",
       " 'referendum': 637,\n",
       " 'jewish': 638,\n",
       " 'stewart': 639,\n",
       " 'string': 640,\n",
       " 'changes': 641,\n",
       " 'survived': 642,\n",
       " 'approval': 643,\n",
       " 'haiti': 644,\n",
       " 'software': 645,\n",
       " 'needs': 646,\n",
       " 'report': 647,\n",
       " 'www': 648,\n",
       " 'baghdad': 649,\n",
       " 'village': 650,\n",
       " 'beaten': 651,\n",
       " 'judges': 652,\n",
       " 'some': 653,\n",
       " 'tools': 654,\n",
       " 'breakaway': 655,\n",
       " 'applied': 656,\n",
       " 'went': 657,\n",
       " 'familiar': 658,\n",
       " 'airlines': 659,\n",
       " 'hundreds': 660,\n",
       " 'slip': 661,\n",
       " 'proved': 662,\n",
       " 'straight': 663,\n",
       " 'gain': 664,\n",
       " 'inflationary': 665,\n",
       " 'practices': 666,\n",
       " 'homer': 667,\n",
       " 'congo': 668,\n",
       " 'jump': 669,\n",
       " 'came': 670,\n",
       " 'threat': 671,\n",
       " 'track': 672,\n",
       " 'disruptions': 673,\n",
       " 'pushed': 674,\n",
       " 'problems': 675,\n",
       " 'disappeared': 676,\n",
       " 'need': 677,\n",
       " 'committee': 678,\n",
       " 'increased': 679,\n",
       " 'schroeder': 680,\n",
       " 'jcp': 681,\n",
       " 'friends': 682,\n",
       " 'french': 683,\n",
       " 'sri': 684,\n",
       " 'figures': 685,\n",
       " 'advanced': 686,\n",
       " 'increasing': 687,\n",
       " 'invoices': 688,\n",
       " 'losing': 689,\n",
       " 'outfielder': 690,\n",
       " 'row': 691,\n",
       " 'recovering': 692,\n",
       " 'hoping': 693,\n",
       " 'isn': 694,\n",
       " 'reds': 695,\n",
       " 'pitched': 696,\n",
       " 'ing': 697,\n",
       " 'range': 698,\n",
       " 'prove': 699,\n",
       " 'played': 700,\n",
       " 'branch': 701,\n",
       " 'sun': 702,\n",
       " 'exporters': 703,\n",
       " 'massacre': 704,\n",
       " 'ohio': 705,\n",
       " 'sense': 706,\n",
       " 'people': 707,\n",
       " 'truce': 708,\n",
       " 'gas': 709,\n",
       " 'billion': 710,\n",
       " 'northern': 711,\n",
       " 'arlington': 712,\n",
       " 'financial': 713,\n",
       " 'pga': 714,\n",
       " 'bargain': 715,\n",
       " 'singh': 716,\n",
       " 'refused': 717,\n",
       " 'media': 718,\n",
       " 'haas': 719,\n",
       " 'urban': 720,\n",
       " 'previous': 721,\n",
       " 'after': 722,\n",
       " 'old': 723,\n",
       " 'improvement': 724,\n",
       " 'continue': 725,\n",
       " 'matter': 726,\n",
       " 'foot': 727,\n",
       " 'trap': 728,\n",
       " 'victory': 729,\n",
       " 'drove': 730,\n",
       " 'republic': 731,\n",
       " 'interim': 732,\n",
       " 'brown': 733,\n",
       " 'parmalat': 734,\n",
       " 'case': 735,\n",
       " 'madrid': 736,\n",
       " 'person': 737,\n",
       " 'heard': 738,\n",
       " 'growing': 739,\n",
       " 'nortel': 740,\n",
       " 'yet': 741,\n",
       " 'burundi': 742,\n",
       " 'equity': 743,\n",
       " 'reforms': 744,\n",
       " 'jays': 745,\n",
       " 'services': 746,\n",
       " 'organization': 747,\n",
       " 'activity': 748,\n",
       " 'ratings': 749,\n",
       " 'awaited': 750,\n",
       " 'champion': 751,\n",
       " 'behind': 752,\n",
       " 'nine': 753,\n",
       " 'initial': 754,\n",
       " 'required': 755,\n",
       " 'denied': 756,\n",
       " 'tumbled': 757,\n",
       " 'basketball': 758,\n",
       " 'collect': 759,\n",
       " 'targets': 760,\n",
       " 'pay': 761,\n",
       " 'step': 762,\n",
       " 'mandate': 763,\n",
       " 'sudanese': 764,\n",
       " 'african': 765,\n",
       " 'cos': 766,\n",
       " 'perhaps': 767,\n",
       " 'cp': 768,\n",
       " 'peace': 769,\n",
       " 'wholesale': 770,\n",
       " 'suspected': 771,\n",
       " 'get': 772,\n",
       " 'suspects': 773,\n",
       " 'why': 774,\n",
       " 'decided': 775,\n",
       " 'ways': 776,\n",
       " 'league': 777,\n",
       " 'despite': 778,\n",
       " 'next': 779,\n",
       " 'around': 780,\n",
       " 'opened': 781,\n",
       " 'monthly': 782,\n",
       " 'freestyle': 783,\n",
       " 'keep': 784,\n",
       " 'silvio': 785,\n",
       " 'boxing': 786,\n",
       " 'association': 787,\n",
       " 'losses': 788,\n",
       " 'ossetia': 789,\n",
       " 'members': 790,\n",
       " 'then': 791,\n",
       " 'dollar': 792,\n",
       " 'pulled': 793,\n",
       " 'billions': 794,\n",
       " 'lost': 795,\n",
       " 'arrested': 796,\n",
       " 'energy': 797,\n",
       " 'tens': 798,\n",
       " 'winged': 799,\n",
       " 'improved': 800,\n",
       " 'cincinnati': 801,\n",
       " 'bronze': 802,\n",
       " 'grand': 803,\n",
       " 'race': 804,\n",
       " 'quot': 805,\n",
       " 'site': 806,\n",
       " 'india': 807,\n",
       " 'bills': 808,\n",
       " 'must': 809,\n",
       " 'concerns': 810,\n",
       " 'criticism': 811,\n",
       " 'finished': 812,\n",
       " 'legal': 813,\n",
       " 'turned': 814,\n",
       " 'twice': 815,\n",
       " 'dozen': 816,\n",
       " 'fired': 817,\n",
       " 'lourdes': 818,\n",
       " 'makers': 819,\n",
       " 'sox': 820,\n",
       " 'mcdonald': 821,\n",
       " 'shrine': 822,\n",
       " 'tied': 823,\n",
       " 'average': 824,\n",
       " 'medical': 825,\n",
       " 'decision': 826,\n",
       " 'commerce': 827,\n",
       " 'kostas': 828,\n",
       " 'fell': 829,\n",
       " 'park': 830,\n",
       " 'investment': 831,\n",
       " 'sardinia': 832,\n",
       " 'amat': 833,\n",
       " 'giving': 834,\n",
       " 'bringing': 835,\n",
       " 'eased': 836,\n",
       " 'began': 837,\n",
       " 'bank': 838,\n",
       " 'fencing': 839,\n",
       " 'terrorism': 840,\n",
       " 'test': 841,\n",
       " 'fre': 842,\n",
       " 'dawn': 843,\n",
       " 'run': 844,\n",
       " 'expansion': 845,\n",
       " 'knee': 846,\n",
       " 'reversed': 847,\n",
       " 'avoid': 848,\n",
       " 'bowl': 849,\n",
       " 'microsoft': 850,\n",
       " 'championships': 851,\n",
       " 'brazil': 852,\n",
       " 'afp': 853,\n",
       " 'annual': 854,\n",
       " 'rebounded': 855,\n",
       " 'sale': 856,\n",
       " 'german': 857,\n",
       " 'new': 858,\n",
       " 'became': 859,\n",
       " 'oil': 860,\n",
       " 'gap': 861,\n",
       " 'starts': 862,\n",
       " 'halliburton': 863,\n",
       " 'port': 864,\n",
       " 'scheduled': 865,\n",
       " 'refugees': 866,\n",
       " 'march': 867,\n",
       " 'early': 868,\n",
       " 'campaign': 869,\n",
       " 'positions': 870,\n",
       " 'work': 871,\n",
       " 'growth': 872,\n",
       " 'times': 873,\n",
       " 'agency': 874,\n",
       " 'wanted': 875,\n",
       " 'other': 876,\n",
       " 'promised': 877,\n",
       " 'den': 878,\n",
       " 'tests': 879,\n",
       " 'sharp': 880,\n",
       " 'settlement': 881,\n",
       " 'authority': 882,\n",
       " 'come': 883,\n",
       " 'replaced': 884,\n",
       " 'crowd': 885,\n",
       " 'suspension': 886,\n",
       " 'change': 887,\n",
       " 'angeles': 888,\n",
       " 'venezuelan': 889,\n",
       " 'returning': 890,\n",
       " 'grew': 891,\n",
       " 'sprint': 892,\n",
       " 'aware': 893,\n",
       " 'innings': 894,\n",
       " 'payments': 895,\n",
       " 'href': 896,\n",
       " 'uprising': 897,\n",
       " 'countries': 898,\n",
       " 'agent': 899,\n",
       " 'group': 900,\n",
       " 'whose': 901,\n",
       " 'venezuela': 902,\n",
       " 'middle': 903,\n",
       " 'tom': 904,\n",
       " 'should': 905,\n",
       " 'klete': 906,\n",
       " 'fullquote': 907,\n",
       " 'farm': 908,\n",
       " 'exchange': 909,\n",
       " 'hands': 910,\n",
       " 'officer': 911,\n",
       " 'percent': 912,\n",
       " 'italy': 913,\n",
       " 'stake': 914,\n",
       " 'registration': 915,\n",
       " 'huge': 916,\n",
       " 'pitcher': 917,\n",
       " 'radical': 918,\n",
       " 'country': 919,\n",
       " 'robert': 920,\n",
       " 'xinhuanet': 921,\n",
       " 'over': 922,\n",
       " 'gains': 923,\n",
       " 'release': 924,\n",
       " 'bhp': 925,\n",
       " 'banned': 926,\n",
       " 'electricity': 927,\n",
       " 'player': 928,\n",
       " 'thing': 929,\n",
       " 'action': 930,\n",
       " 'information': 931,\n",
       " 'borders': 932,\n",
       " 'residents': 933,\n",
       " 'thorpe': 934,\n",
       " 'bay': 935,\n",
       " 'clemens': 936,\n",
       " 'production': 937,\n",
       " 'kosuke': 938,\n",
       " 'midday': 939,\n",
       " 'wounding': 940,\n",
       " 'inning': 941,\n",
       " 'dispute': 942,\n",
       " 'authorities': 943,\n",
       " 'under': 944,\n",
       " 'hour': 945,\n",
       " 'numbers': 946,\n",
       " 'ended': 947,\n",
       " 'fighting': 948,\n",
       " 'sprinter': 949,\n",
       " 'school': 950,\n",
       " 'presidential': 951,\n",
       " 'semifinals': 952,\n",
       " 'win': 953,\n",
       " 'season': 954,\n",
       " 'vijay': 955,\n",
       " 'go': 956,\n",
       " 'match': 957,\n",
       " 'forces': 958,\n",
       " 'networks': 959,\n",
       " 'delay': 960,\n",
       " 'google': 961,\n",
       " 'federal': 962,\n",
       " 'fractured': 963,\n",
       " 'rates': 964,\n",
       " 'american': 965,\n",
       " 'friendly': 966,\n",
       " 'de': 967,\n",
       " 'following': 968,\n",
       " 'credit': 969,\n",
       " 'james': 970,\n",
       " 'best': 971,\n",
       " 'effort': 972,\n",
       " 'un': 973,\n",
       " 'rebels': 974,\n",
       " 'dow': 975,\n",
       " 'ukraine': 976,\n",
       " 'offered': 977,\n",
       " 'performance': 978,\n",
       " 'kevin': 979,\n",
       " 'boston': 980,\n",
       " 'course': 981,\n",
       " 'suspicious': 982,\n",
       " 'phillies': 983,\n",
       " 'cold': 984,\n",
       " 'driven': 985,\n",
       " 'rival': 986,\n",
       " 'drug': 987,\n",
       " 'war': 988,\n",
       " 'depot': 989,\n",
       " 'crisis': 990,\n",
       " 'ny': 991,\n",
       " 'ending': 992,\n",
       " 'real': 993,\n",
       " 'vs': 994,\n",
       " 'easing': 995,\n",
       " 'pool': 996,\n",
       " 'separate': 997,\n",
       " 'conference': 998,\n",
       " 'equipment': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a vocab_id to word dictionary\n",
    "vocab_idtoword = dict(enumerate(vocab))\n",
    "\n",
    "#creating a new dictionary where the keys are the words from vocab\n",
    "wordtovocab_id = {i:w for w, i in vocab_idtoword.items()}\n",
    "\n",
    "wordtovocab_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix. \n",
    "\n",
    "First, represent documents in train, dev and test sets as lists of words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.047887Z",
     "start_time": "2020-04-02T14:26:44.920631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_vocab_indices(vocab, word_to_id):\n",
    "    \n",
    "    # Initialize the list of vocabulary indices for each document\n",
    "    vocab_indices = []\n",
    "    # Convert the input vocabulary into a list of documents\n",
    "    doc_list = list(vocab)\n",
    "    \n",
    "    # Loop over each document\n",
    "    for i in range(len(doc_list)):\n",
    "        # Initialize the list of indices for the current document\n",
    "        indices_list = []\n",
    "        # Loop over each word in the current document\n",
    "        for word in doc_list[i]:\n",
    "            # If the word is in the word-to-id mapping, add its id to the list of indices\n",
    "            if word in word_to_id:\n",
    "                indices_list.append(word_to_id[word])\n",
    "        # Add the list of indices for the current document to the list of vocabulary indices\n",
    "        vocab_indices.append(indices_list)\n",
    "    \n",
    "    # Return the list of documents and the corresponding list of vocabulary indices\n",
    "    return doc_list, vocab_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create_vocab_indices\n",
    "\n",
    "This function takes in two arguments: vocab and wordtovocab_id.\n",
    "\n",
    "vocab is a set of words in a list format, where each element of the list represents a document.\n",
    "wordtovocab_id is a dictionary where the keys are the words in the vocabulary and the values are the corresponding indices of those words.\n",
    "The function returns two values:\n",
    "\n",
    "uni_list - A list of lists where each element represents a document from the input vocab list but with all its words converted into their respective indices using the wordtovocab_id dictionary.\n",
    "vocab_indices - A list of lists where each inner list contains the indices of all the words present in the corresponding document of the vocab list.\n",
    "\n",
    "The function then does a for loop iteration over each document in the vocabulary list before doing another for loop iteration over each word in the list. The function determines whether the current word is in the wordtovocab_id dictionary inside the inner loop, and if it is, it obtains the appropriate index and adds the word to the list_vocab.\n",
    "\n",
    "The function adds list_vocab to vocab_indices after checking every word in the document. The result of the function is the uni_list and vocab_indices after this process is performed for each document in the vocabulary list.\n",
    "\n",
    "A list of documents with words can be changed into a list of documents with the matching word indices using this function, which can then be used as input for a neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert them into lists of indices in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.752658Z",
     "start_time": "2020-04-02T14:26:45.730409Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# processing all vocab to make documents(list of indices)\n",
    "\n",
    "vocab_train, vocab_dev, vocab_test = [],[],[]\n",
    "for text in train_text:\n",
    "    vocab_train.append(extract_ngrams(text, ngram_range=(1,1),stop_words=stop_words))\n",
    "for text in dev_text:\n",
    "    vocab_dev.append(extract_ngrams(text, ngram_range=(1,1),stop_words=stop_words))\n",
    "for text in test_text:\n",
    "    vocab_test.append(extract_ngrams(text, ngram_range=(1,1),stop_words=stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:03:13.183996Z",
     "start_time": "2020-04-02T15:03:13.077575Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_doc_tr,X_tr = create_vocab_indices(vocab_train,wordtovocab_id)\n",
    "X_doc_dev,X_dev = create_vocab_indices(vocab_dev,wordtovocab_id)\n",
    "X_doc_test,X_test = create_vocab_indices(vocab_test,wordtovocab_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(X_uni_tr)\n",
    "#print(X_tr)\n",
    "len(X_tr)\n",
    "\n",
    "#Checking the training data for the News Articles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the labels `Y` for train, dev and test sets into arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the label file\n",
    "train_class = np.array(train_df['Class'])\n",
    "dev_class = np.array(dev_df['Class'])\n",
    "test_class = np.array(test_df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_empty(data, label):\n",
    "    \"\"\"\n",
    "    Deletes empty elements in the data and corresponding labels.\n",
    "    \n",
    "    \"\"\"\n",
    "    label_no_empty = []\n",
    "    data_no_empty = []\n",
    "    for i in range(len(data)):\n",
    "        if len(data[i]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            data_no_empty.append(data[i])\n",
    "            label_no_empty.append(label[i])\n",
    "            \n",
    "    return data_no_empty, np.array(label_no_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr,train_class = delete_empty(X_tr,train_class)\n",
    "X_dev,dev_class = delete_empty(X_dev,dev_class)\n",
    "X_test,test_class = delete_empty(X_test,test_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete_empty function\n",
    "\n",
    "While creating the forward pass function and the SGD function , it was observed that one of the list in the testing set had length 0 ,to remove any unwanted 0 size list , this function is created.\n",
    "\n",
    "A list of lists containing data elements and a list of labels identifying those data elements are input into this function. After that, it loops through the list of data pieces to see if any of them are empty. It moves on to the following element if it encounters an empty element. If it does, it adds the non-empty element to a list named data_no_empty and adds the label for that element to a list called label_no_empty. The two new lists, data_no_empty and label_no_empty, are the function's final outputs.\n",
    "\n",
    "This helps to remove any length 0 list in testing set so that the forward pass function can work efficiently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tr = train_class\n",
    "Y_dev = dev_class\n",
    "Y_te = test_class\n",
    "X_te = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 3 3 3]\n",
      "2400\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(Y_tr)\n",
    "print(len(Y_tr))\n",
    "print(type(Y_tr))\n",
    "\n",
    "#Checking the training data for the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "\n",
    "Your network should pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "During training, $\\mathbf{a}_1$ should be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "You can extend to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initiliasing the weight matrices. For that purpose, you should implement the `network_weights` function that takes as input:\n",
    "\n",
    "- `vocab_size`: the size of the vocabulary\n",
    "- `embedding_dim`: the size of the word embeddings\n",
    "- `hidden_dim`: a list of the sizes of any subsequent hidden layers. Empty if there are no hidden layers between the average embedding and the output layer \n",
    "- `num_classes`: the number of the classes for the output layer\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: a dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers (hint: use numpy.random.uniform with from -0.1 to 0.1)\n",
    "\n",
    "Make sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise you won't be able to perform forward and backward passes. Consider also using np.float32 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:41:20.918617Z",
     "start_time": "2020-04-02T15:41:20.915597Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=2000, \n",
    "                    embedding_dim=300, \n",
    "                    hidden_dim=[], \n",
    "                    num_classes=3, \n",
    "                    init_val = 0.5):\n",
    "    \n",
    "    # Define a list to hold the size of each layer in the network\n",
    "    # Start with the size of the input layer (vocabulary size) and \n",
    "    # add the size of the embedding layer (embedding_dim)\n",
    "    \n",
    "    list_of_layer = [vocab_size, embedding_dim]\n",
    "    \n",
    "    # Add the size of the hidden layers specified by the `hidden_dim` argument\n",
    "    for i in hidden_dim:\n",
    "        list_of_layer.append(i)\n",
    "        \n",
    "    # Finally, add the size of the output layer (number of classes)\n",
    "    list_of_layer.append(num_classes) \n",
    "    # print(list_of_layer)\n",
    "    \n",
    "    # Initialize the weights dictionary\n",
    "    W = dict()\n",
    "    \n",
    "    # Iterate over the layers and initialize the weights\n",
    "    for id_layer in range(len(list_of_layer) -1):\n",
    "        W[id_layer] = np.random.uniform(-init_val, init_val,\n",
    "                                        (list_of_layer[id_layer],list_of_layer[id_layer +1])).astype(\"float32\")\n",
    "            \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:48.636732Z",
     "start_time": "2020-04-02T14:26:48.634122Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[ 0.19646919, -0.21386066, -0.27314854, ...,  0.34605485,\n",
       "         -0.376077  ,  0.0964869 ],\n",
       "        [-0.48360753,  0.22118437, -0.49226248, ...,  0.04941306,\n",
       "         -0.47245708, -0.468082  ],\n",
       "        [ 0.20135981,  0.20758112,  0.45993912, ...,  0.34579822,\n",
       "         -0.11632725, -0.43926036],\n",
       "        ...,\n",
       "        [ 0.08108832, -0.04608797, -0.01016513, ..., -0.481566  ,\n",
       "         -0.20555991,  0.44186756],\n",
       "        [ 0.19522132, -0.02899734,  0.03076438, ...,  0.4761879 ,\n",
       "          0.22056226, -0.02757094],\n",
       "        [ 0.02454556, -0.13820393, -0.40596458, ..., -0.17762911,\n",
       "         -0.24093711,  0.40163487]], dtype=float32),\n",
       " 1: array([[-0.17465581, -0.06111839, -0.4696872 ],\n",
       "        [-0.40016612,  0.3465482 , -0.39759788],\n",
       "        [ 0.3670038 , -0.18756968,  0.17128825],\n",
       "        [-0.10109809, -0.28860265,  0.26096398],\n",
       "        [-0.24831566,  0.19033252, -0.33913198],\n",
       "        [ 0.39314458, -0.43271306,  0.43780908],\n",
       "        [-0.42476106, -0.12251305, -0.10697404],\n",
       "        [ 0.10237579,  0.2695777 , -0.2816682 ],\n",
       "        [ 0.2894114 ,  0.2396454 ,  0.27998638],\n",
       "        [ 0.0072607 , -0.36016488, -0.37356466],\n",
       "        [ 0.37037477,  0.48161572, -0.28341383],\n",
       "        [ 0.02518826,  0.3185745 , -0.46291918],\n",
       "        [-0.4019993 , -0.37678242,  0.35742334],\n",
       "        [ 0.3033177 ,  0.4964052 , -0.32575125],\n",
       "        [-0.2106773 , -0.40045866, -0.21172833],\n",
       "        [ 0.46571067, -0.13448916, -0.45205155],\n",
       "        [ 0.27320927, -0.11118611,  0.39509052],\n",
       "        [-0.3195149 ,  0.20021811,  0.27266335],\n",
       "        [-0.4962583 ,  0.15024409, -0.01907124],\n",
       "        [ 0.15856373, -0.20289679,  0.354096  ],\n",
       "        [-0.35411397, -0.18218815,  0.33787802],\n",
       "        [ 0.48753294, -0.38863903, -0.27770033],\n",
       "        [ 0.09743774,  0.05433511,  0.2157628 ],\n",
       "        [ 0.23558453,  0.01053063, -0.2866864 ],\n",
       "        [-0.0396763 , -0.39464694,  0.16965932],\n",
       "        [-0.40742263,  0.29316887,  0.14802407],\n",
       "        [-0.07467962, -0.03545059, -0.0564972 ],\n",
       "        [-0.23492356,  0.02102305,  0.23146778],\n",
       "        [ 0.41119906,  0.04621457,  0.36333746],\n",
       "        [ 0.14420246,  0.24073058,  0.18661633],\n",
       "        [ 0.29309848, -0.21135233, -0.0708115 ],\n",
       "        [-0.1603053 ,  0.4281182 ,  0.1503635 ],\n",
       "        [-0.4160575 ,  0.2377481 ,  0.10516784],\n",
       "        [-0.4992382 , -0.11251452,  0.06161881],\n",
       "        [-0.3710008 , -0.18750913, -0.38702357],\n",
       "        [ 0.24706374, -0.18099344, -0.21166785],\n",
       "        [-0.39348522, -0.00878966,  0.36888903],\n",
       "        [-0.4814923 , -0.08543836,  0.25217572],\n",
       "        [ 0.44136164,  0.45993325,  0.03844387],\n",
       "        [ 0.01067059,  0.2521008 , -0.12589766],\n",
       "        [-0.1694113 , -0.10568538, -0.44148403],\n",
       "        [-0.38642406,  0.3884843 ,  0.25409856],\n",
       "        [-0.00930582, -0.09442578,  0.36184412],\n",
       "        [ 0.41463292,  0.03178057, -0.46186343],\n",
       "        [ 0.3249844 ,  0.10067846,  0.25446582],\n",
       "        [ 0.28904906, -0.28870678, -0.36079222],\n",
       "        [ 0.08821215,  0.32209778,  0.41982004],\n",
       "        [-0.13037215,  0.32016858,  0.3249971 ],\n",
       "        [-0.26110718, -0.19864899,  0.3771655 ],\n",
       "        [ 0.33100933,  0.43318996,  0.16544609],\n",
       "        [ 0.48592404, -0.29741865,  0.17658325],\n",
       "        [ 0.25706667,  0.33545104,  0.139119  ],\n",
       "        [ 0.27869648,  0.13141823, -0.34718132],\n",
       "        [-0.27717614, -0.4744515 , -0.39438015],\n",
       "        [-0.02861442,  0.49961743, -0.24093793],\n",
       "        [ 0.08880151,  0.14796779,  0.43779695],\n",
       "        [-0.0958064 ,  0.44660142,  0.3362052 ],\n",
       "        [ 0.26613894, -0.26605722, -0.41271242],\n",
       "        [ 0.09549198,  0.3453419 ,  0.36011484],\n",
       "        [-0.00215778,  0.28117448, -0.07026198],\n",
       "        [-0.22000904,  0.02587325,  0.35910073],\n",
       "        [ 0.38615498, -0.43909594,  0.09108137],\n",
       "        [ 0.30858907, -0.19823721, -0.27877071],\n",
       "        [ 0.4383027 ,  0.12000742,  0.30500817],\n",
       "        [ 0.03850316,  0.09383225, -0.0158395 ],\n",
       "        [-0.1206841 , -0.10888293, -0.07919695],\n",
       "        [-0.02931115,  0.2553232 ,  0.46680647],\n",
       "        [-0.16609454, -0.00385533, -0.07349507],\n",
       "        [-0.4223348 , -0.26008105, -0.4736779 ],\n",
       "        [ 0.4393391 , -0.30684105,  0.12384166],\n",
       "        [-0.02160788, -0.2655489 , -0.10642345],\n",
       "        [-0.1356753 ,  0.37715757, -0.06375719],\n",
       "        [-0.32271674,  0.3780352 ,  0.02741536],\n",
       "        [-0.01758109, -0.05997236, -0.22321595],\n",
       "        [ 0.30389607,  0.2807669 , -0.34166533],\n",
       "        [-0.1529916 , -0.12249786,  0.01340461],\n",
       "        [-0.43063724,  0.04717811,  0.09394048],\n",
       "        [ 0.12344533,  0.21698524, -0.3934708 ],\n",
       "        [ 0.05656289,  0.35245806,  0.39825556],\n",
       "        [-0.4509147 , -0.38874012,  0.22050092],\n",
       "        [ 0.4925275 , -0.08096074, -0.37476537],\n",
       "        [ 0.10525972, -0.07414307,  0.06968495],\n",
       "        [ 0.1582095 ,  0.05337281, -0.32763305],\n",
       "        [-0.17082171,  0.38185248,  0.21836518],\n",
       "        [ 0.34886658, -0.4040761 , -0.36642644],\n",
       "        [ 0.37415397, -0.08265737, -0.23061152],\n",
       "        [ 0.32481855, -0.3298984 , -0.27862948],\n",
       "        [ 0.24926405, -0.07785861, -0.49379727],\n",
       "        [-0.13075572,  0.44976005, -0.44175148],\n",
       "        [-0.07877092, -0.19199923,  0.38563105],\n",
       "        [-0.00784841,  0.41227067,  0.04086814],\n",
       "        [ 0.3463408 , -0.23888879, -0.09129914],\n",
       "        [ 0.2276093 ,  0.35355258, -0.19168928],\n",
       "        [-0.12890443,  0.12489396,  0.1182256 ],\n",
       "        [ 0.26336917,  0.16624124,  0.10573903],\n",
       "        [ 0.21790981, -0.215271  ,  0.4127618 ],\n",
       "        [-0.37342182, -0.17290571,  0.1646145 ],\n",
       "        [-0.13256869, -0.1877755 , -0.30604154],\n",
       "        [ 0.04296459, -0.30829233,  0.04662165],\n",
       "        [-0.2779435 , -0.02956882,  0.34542108],\n",
       "        [-0.06247269,  0.23000704, -0.191093  ],\n",
       "        [-0.29013476,  0.11708868,  0.48842183],\n",
       "        [-0.11100549, -0.4905545 ,  0.11374731],\n",
       "        [-0.4675246 ,  0.2565275 ,  0.12907231],\n",
       "        [ 0.33332857, -0.11343259,  0.3474142 ],\n",
       "        [ 0.29147872,  0.43221244, -0.04929972],\n",
       "        [ 0.49821988, -0.15055576, -0.07678368],\n",
       "        [ 0.06683372,  0.15502901,  0.41463268],\n",
       "        [-0.0636429 , -0.41512498, -0.21096112],\n",
       "        [ 0.12493771,  0.3034379 , -0.38205844],\n",
       "        [-0.01645482, -0.3702034 ,  0.26489407],\n",
       "        [-0.10628252,  0.49381912,  0.3042125 ],\n",
       "        [ 0.2908457 , -0.14892283, -0.17705646],\n",
       "        [ 0.25440073,  0.0343494 , -0.43555278],\n",
       "        [-0.08567578,  0.16302535,  0.37682703],\n",
       "        [-0.263536  ,  0.14536037, -0.40382764],\n",
       "        [-0.3539514 ,  0.4886267 , -0.37122348],\n",
       "        [ 0.05633373, -0.18465838,  0.12011731],\n",
       "        [-0.15716855, -0.27891138,  0.07624409],\n",
       "        [-0.3553232 , -0.13530865, -0.49244255],\n",
       "        [-0.454751  ,  0.0357024 , -0.3870413 ],\n",
       "        [-0.01271359,  0.44658077,  0.49561098],\n",
       "        [-0.49828663, -0.31512722,  0.2747573 ],\n",
       "        [ 0.45943558, -0.1278682 ,  0.30009508],\n",
       "        [-0.05493442, -0.47331634,  0.05152392],\n",
       "        [ 0.12869632, -0.27147108,  0.18795629],\n",
       "        [-0.33045977,  0.47522512,  0.15042363],\n",
       "        [-0.24650283, -0.11748676,  0.37598303],\n",
       "        [-0.09275464,  0.03601478, -0.0608447 ],\n",
       "        [-0.45417556,  0.22189732,  0.26314923],\n",
       "        [-0.3285758 ,  0.20227821, -0.47299388],\n",
       "        [-0.11969484,  0.11162319,  0.02042083],\n",
       "        [-0.4482991 , -0.35741857,  0.32596764],\n",
       "        [-0.05156212, -0.1934062 , -0.22319962],\n",
       "        [-0.24195938, -0.47238675,  0.46143046],\n",
       "        [ 0.02318092, -0.45251194, -0.43139303],\n",
       "        [ 0.14223053,  0.47381666,  0.41372678],\n",
       "        [-0.14915732, -0.00770148,  0.17318782],\n",
       "        [ 0.42349112, -0.06588558,  0.43850434],\n",
       "        [-0.06106096,  0.4773389 , -0.4323745 ],\n",
       "        [-0.2865871 ,  0.4057888 ,  0.4715665 ],\n",
       "        [ 0.32480764,  0.39055943, -0.35924166],\n",
       "        [-0.36744604, -0.04571299,  0.38272312],\n",
       "        [-0.31262016, -0.0041085 , -0.3271269 ],\n",
       "        [ 0.4906239 , -0.4442139 , -0.1718426 ],\n",
       "        [-0.43703806,  0.06963243,  0.17777592],\n",
       "        [-0.22296904, -0.25758678,  0.24522816],\n",
       "        [ 0.35074794,  0.48358357,  0.43014956],\n",
       "        [-0.15821417, -0.14749756,  0.38720593],\n",
       "        [ 0.24869047,  0.19208337, -0.09635431],\n",
       "        [ 0.21454068, -0.4713465 ,  0.37849897],\n",
       "        [ 0.44600722, -0.14824532,  0.07459996],\n",
       "        [ 0.43703702, -0.1149478 ,  0.4377723 ],\n",
       "        [ 0.1883965 , -0.09931582,  0.10977363],\n",
       "        [-0.3672877 ,  0.4767487 , -0.35063055],\n",
       "        [-0.32701778, -0.4967758 ,  0.48772684],\n",
       "        [-0.0809847 , -0.14881642, -0.04988572],\n",
       "        [ 0.09739836,  0.2306179 , -0.22092988],\n",
       "        [ 0.06453332, -0.4618093 ,  0.24253446],\n",
       "        [ 0.3277334 ,  0.09523882,  0.44358978],\n",
       "        [-0.47162452, -0.44061357, -0.11219527],\n",
       "        [ 0.38906428,  0.22835425, -0.40859506],\n",
       "        [-0.28392646,  0.0526468 ,  0.2570443 ],\n",
       "        [-0.11530395, -0.08591396,  0.04890569],\n",
       "        [ 0.26168567, -0.49545243,  0.32701445],\n",
       "        [-0.48196846, -0.2027681 ,  0.10853297],\n",
       "        [-0.12606731,  0.05816259, -0.3380912 ],\n",
       "        [ 0.40024006, -0.07633648,  0.25377122],\n",
       "        [ 0.2820957 , -0.01045185,  0.2652224 ],\n",
       "        [-0.079115  ,  0.13680328, -0.36749995],\n",
       "        [-0.01791257, -0.17628577,  0.42161328],\n",
       "        [-0.313239  , -0.42600688, -0.27595448],\n",
       "        [-0.2712754 , -0.33660543,  0.03780549],\n",
       "        [-0.07742944, -0.23405206, -0.05352797],\n",
       "        [ 0.14537784,  0.15915674,  0.16494274],\n",
       "        [-0.39004475, -0.06745032, -0.17807393],\n",
       "        [ 0.3412176 , -0.30838346,  0.20452608],\n",
       "        [-0.03030112,  0.21511064,  0.09717758],\n",
       "        [-0.13651143, -0.28726432,  0.24713701],\n",
       "        [-0.3062164 ,  0.00065071, -0.43810803],\n",
       "        [-0.4053468 ,  0.1886482 ,  0.08947641],\n",
       "        [-0.09633884,  0.33496323, -0.08173046],\n",
       "        [-0.00578801,  0.17534362, -0.48970997],\n",
       "        [-0.20720217,  0.05235654,  0.30522564],\n",
       "        [-0.35406458,  0.45569688, -0.2002567 ],\n",
       "        [ 0.17940539,  0.08866059,  0.3718703 ],\n",
       "        [ 0.00873315,  0.30034652, -0.0535507 ],\n",
       "        [-0.30973014, -0.13704534, -0.33540183],\n",
       "        [ 0.4529601 , -0.29800934,  0.04096342],\n",
       "        [-0.3831516 , -0.07468665,  0.24125572],\n",
       "        [-0.22436678, -0.28986064,  0.29353392],\n",
       "        [ 0.12950157,  0.08347119, -0.19383661],\n",
       "        [-0.28082654, -0.4421552 , -0.07922448],\n",
       "        [ 0.38787323,  0.36326393, -0.06870244],\n",
       "        [-0.498412  ,  0.23340942, -0.01893093],\n",
       "        [-0.02353635, -0.4307205 ,  0.2641254 ],\n",
       "        [ 0.20388383, -0.03918057, -0.17726575],\n",
       "        [-0.03068743, -0.4148114 ,  0.2320792 ],\n",
       "        [-0.20048846,  0.35012794, -0.39284056],\n",
       "        [-0.09242862,  0.49615663, -0.03382326],\n",
       "        [ 0.31190905,  0.2951092 , -0.12648286],\n",
       "        [-0.08838225, -0.45286217, -0.49231094],\n",
       "        [-0.16267411,  0.4498609 , -0.03999586],\n",
       "        [-0.37911218, -0.27656838, -0.10620528],\n",
       "        [-0.44429943,  0.49018717,  0.4808955 ],\n",
       "        [ 0.22013177,  0.4038213 , -0.06646896],\n",
       "        [-0.20464928, -0.3263795 ,  0.1259076 ],\n",
       "        [-0.10142417,  0.26142743,  0.33302876],\n",
       "        [-0.30433178,  0.08084474,  0.22455303],\n",
       "        [-0.04781009,  0.41899222,  0.33168328],\n",
       "        [-0.23540899, -0.26941213,  0.28719229],\n",
       "        [-0.08124934,  0.21573891, -0.35962185],\n",
       "        [-0.4425267 ,  0.30503646, -0.40129417],\n",
       "        [ 0.40414068,  0.32907274,  0.14045109],\n",
       "        [-0.07032523, -0.4991006 ,  0.44458282],\n",
       "        [ 0.31498578, -0.17299856, -0.42037556],\n",
       "        [ 0.23149258,  0.25897333, -0.15391049],\n",
       "        [ 0.12056574, -0.19620237, -0.32967484],\n",
       "        [ 0.4914293 ,  0.31458923, -0.35828748],\n",
       "        [ 0.3288428 , -0.19511484,  0.25935715],\n",
       "        [ 0.48545903,  0.23436628, -0.36396718],\n",
       "        [ 0.4788201 ,  0.28091162,  0.4163697 ],\n",
       "        [-0.01217389, -0.41086638,  0.26177955],\n",
       "        [-0.44168442,  0.12280365,  0.2203171 ],\n",
       "        [ 0.09359346, -0.08842234,  0.36845395],\n",
       "        [-0.4423439 , -0.39157805, -0.19408001],\n",
       "        [ 0.25541583,  0.03283333, -0.22083549],\n",
       "        [-0.25435355,  0.08148849, -0.2803547 ],\n",
       "        [ 0.36486724, -0.32265246, -0.27783543],\n",
       "        [-0.2917027 , -0.43944812, -0.31282413],\n",
       "        [-0.0149707 ,  0.14516777,  0.46555293],\n",
       "        [-0.42774528, -0.30668062,  0.1774914 ],\n",
       "        [-0.35566017, -0.2633804 , -0.00127884],\n",
       "        [ 0.45739934,  0.18748383, -0.28956553],\n",
       "        [ 0.3887255 , -0.21139427,  0.36324763],\n",
       "        [-0.44129542,  0.08425905, -0.14337368],\n",
       "        [ 0.29732764, -0.27115494,  0.24897318],\n",
       "        [-0.17975396, -0.47474587, -0.2586027 ],\n",
       "        [ 0.20231609,  0.07897799, -0.37422267],\n",
       "        [ 0.02458124,  0.37713268,  0.09176432],\n",
       "        [-0.4365952 ,  0.491329  , -0.0218647 ],\n",
       "        [ 0.27814552, -0.11040536, -0.38664153],\n",
       "        [ 0.1730765 , -0.2463676 , -0.22989021],\n",
       "        [-0.28605402, -0.4937272 ,  0.1403093 ],\n",
       "        [ 0.08775395, -0.14472803,  0.37862968],\n",
       "        [-0.24963982, -0.31707036, -0.03850818],\n",
       "        [-0.30596995,  0.38761652,  0.14924097],\n",
       "        [ 0.21526693, -0.3203269 ,  0.18845157],\n",
       "        [-0.08737068,  0.06389571,  0.25332162],\n",
       "        [ 0.39363045, -0.29832664, -0.3607481 ],\n",
       "        [ 0.13359915,  0.24157782,  0.28413305],\n",
       "        [ 0.053216  , -0.096528  ,  0.36487654],\n",
       "        [-0.31142744,  0.16130306, -0.04334792],\n",
       "        [-0.40592396,  0.10190449, -0.21511014],\n",
       "        [-0.11741855, -0.2666094 , -0.15461484],\n",
       "        [ 0.1845554 ,  0.4163492 ,  0.1396141 ],\n",
       "        [ 0.3838635 , -0.01157106,  0.38378045],\n",
       "        [-0.28917333, -0.35325146, -0.12815812],\n",
       "        [ 0.24802555, -0.37413913,  0.20473754],\n",
       "        [-0.26491764,  0.3765153 , -0.255642  ],\n",
       "        [ 0.39741752, -0.242882  ,  0.1595755 ],\n",
       "        [ 0.24666083,  0.44400546, -0.25063527],\n",
       "        [-0.35876444,  0.3130353 ,  0.05725993],\n",
       "        [ 0.2331764 ,  0.31194153,  0.19940181],\n",
       "        [ 0.44780535,  0.14314531, -0.32466242],\n",
       "        [-0.02352623,  0.02517309, -0.4242067 ],\n",
       "        [-0.4289797 , -0.09864564,  0.45403937],\n",
       "        [-0.31871423, -0.45527354,  0.17365633],\n",
       "        [ 0.39546832, -0.32486445, -0.45537883],\n",
       "        [-0.1829683 ,  0.12779066,  0.18906203],\n",
       "        [-0.25238374, -0.01235093, -0.11351482],\n",
       "        [-0.42880797, -0.3172408 ,  0.0567129 ],\n",
       "        [ 0.43876252, -0.2712393 ,  0.25697413],\n",
       "        [-0.2519418 ,  0.463916  ,  0.2939323 ],\n",
       "        [ 0.3038526 ,  0.44738054, -0.45428607],\n",
       "        [-0.36112714, -0.29928583, -0.32784244],\n",
       "        [-0.13145483,  0.25503868, -0.37071285],\n",
       "        [ 0.3496126 ,  0.4613393 ,  0.10135645],\n",
       "        [ 0.26208037,  0.28225318, -0.05790905],\n",
       "        [-0.07889367,  0.35512438,  0.1735925 ],\n",
       "        [ 0.40823525,  0.21953672, -0.42107296],\n",
       "        [-0.09106737, -0.13102473,  0.07228313],\n",
       "        [ 0.0910496 ,  0.00853058,  0.21734768],\n",
       "        [ 0.28569826,  0.30894253,  0.10170867],\n",
       "        [ 0.271871  , -0.01331332,  0.200761  ],\n",
       "        [-0.45529673,  0.24145861,  0.3741751 ],\n",
       "        [ 0.36681762, -0.15391085,  0.36912376],\n",
       "        [-0.29803988,  0.04173325, -0.4190061 ],\n",
       "        [-0.43725368, -0.06347871, -0.02332703],\n",
       "        [-0.17419012, -0.15470353,  0.13074529],\n",
       "        [-0.2408023 ,  0.33951113, -0.4474142 ],\n",
       "        [-0.02457705,  0.31718585, -0.49177974],\n",
       "        [-0.21732442,  0.06475203,  0.0955325 ],\n",
       "        [-0.17848179,  0.20271938, -0.06578492],\n",
       "        [-0.47140944,  0.27176473,  0.13455532],\n",
       "        [ 0.09232859,  0.14278053,  0.11672509],\n",
       "        [-0.19561265, -0.35213792,  0.04952338],\n",
       "        [ 0.31404588, -0.16219775,  0.4610547 ],\n",
       "        [-0.46005508,  0.02299332,  0.18673874],\n",
       "        [ 0.16002364, -0.44106308, -0.16392617]], dtype=float32)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = network_weights(vocab_size=2000,embedding_dim=300,hidden_dim=[], num_classes=3)\n",
    "W[0].shape\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Weights\n",
    "\n",
    "The weight matrices for our neural network are initialised using this function. The weight matrices are kept in a dictionary, with the keys denoting the network layers and the values denoting the weights connected to each layer.\n",
    "\n",
    "The function initially generates a list of layer sizes that includes the input layer size (vocab_size), the embedding layer size (embedding_dim), and any sizes for hidden layers that are supplied in the hidden_dim argument. The output layer's size (num_classes) is then added as a final component to the list.\n",
    "\n",
    "The values are then randomly initialised weight matrices for each layer using the np.random.uniform() function, and a dictionary W is initialised with keys corresponding to each layer of the network (aside from the input layer). The initialization range for the weight matrices is between -init_val and init_val.\n",
    "\n",
    "\n",
    "The function returns a  W dictionary, which contains the weight matrices for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:31:57.970152Z",
     "start_time": "2020-04-01T10:31:57.966123Z"
    }
   },
   "source": [
    "Then you need to develop a `softmax` function (same as in Assignment 1) to be used in the output layer. \n",
    "\n",
    "It takes as input `z` (array of real numbers) and returns `sig` (the softmax of `z`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    upper = np.exp(z-np.max(z))\n",
    "    bottom = np.sum(np.exp(z-np.max(z)))\n",
    "    sig = upper / bottom    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax \n",
    "\n",
    "This function takes in 'z' which is the output layer of the neural network and returns a softmax of the input vector 'z' which is basically a probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the softmax function\n",
    "sig = softmax([5,6,7])\n",
    "sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the categorical cross entropy loss by slightly modifying the function from Assignment 1 to depend only on the true label `y` and the class probabilities vector `y_preds`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.360838Z",
     "start_time": "2020-04-02T14:26:51.356935Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def categorical_loss(y, y_preds):\n",
    "    l = -np.log(y_preds[y])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorical_loss\n",
    "\n",
    "This function calculates the categorical cross-entropy loss between the true label y and predicted probabilities y_preds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.995732273553991"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = categorical_loss(1,[1,20,2])\n",
    "l\n",
    "#checking the categorical_loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:02:56.149535Z",
     "start_time": "2020-03-31T15:02:56.145738Z"
    }
   },
   "source": [
    "Then, implement the `relu` function to introduce non-linearity after each hidden layer of your network \n",
    "(during the forward pass): \n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "and the `relu_derivative` function to compute its derivative (used in the backward pass):\n",
    "\n",
    "  \n",
    "  relu_derivative($z_i$)=0, if $z_i$<=0, 1 otherwise.\n",
    "  \n",
    "\n",
    "\n",
    "Note that both functions take as input a vector $z$ \n",
    "\n",
    "Hint use .copy() to avoid in place changes in array z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:52.665236Z",
     "start_time": "2020-04-02T14:26:52.661519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    z = z.copy()\n",
    "    a = np.maximum(0, z)    \n",
    "    return a\n",
    "    \n",
    "def relu_derivative(z):\n",
    "    dz=np.array(z)\n",
    "    dz = dz.copy()\n",
    "    dz[dz <= 0] = 0\n",
    "    dz[dz > 0] = 1  \n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu function\n",
    "\n",
    "The ReLU function returns the maximum of 0 and its input z\n",
    "The ReLU derivative function computes the gradient of the ReLU function with respect to its input z. It returns 1 for input values greater than 0 and 0 for values less than or equal to 0:\n",
    "These functions are commonly used as activation functions in neural networks because they are simple and computationally efficient, and can help to mitigate the vanishing gradient problem that can occur with other activation functions like sigmoid and tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n"
     ]
    }
   ],
   "source": [
    "#checking the relu function\n",
    "\n",
    "abc= relu_derivative([1,-2,-3])\n",
    "print(abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training you should also apply a dropout mask element-wise after the activation function (i.e. vector of ones with a random percentage set to zero). The `dropout_mask` function takes as input:\n",
    "\n",
    "- `size`: the size of the vector that we want to apply dropout\n",
    "- `dropout_rate`: the percentage of elements that will be randomly set to zeros\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `dropout_vec`: a vector with binary values (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.429192Z",
     "start_time": "2020-04-02T14:26:53.425301Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    # Initialize a vector of 1\n",
    "    dropout_vec = np.ones(size)\n",
    "\n",
    "    dropout_vec[:round(size*dropout_rate)] = 0.0\n",
    "    np.random.shuffle(dropout_vec)\n",
    "    return dropout_vec\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout_mask\n",
    "\n",
    "This function generates a dropout mask for a given layer size and dropout rate.\n",
    "\n",
    "The function creates a vector of ones with the specified size as its initial value, sets the first round(size*dropout_rate) elements to 0, and then randomly shuffles the vector to produce a random mask for the neurons that will be dropped out. This function's code can be written as the following in a single line:\n",
    "\n",
    "In order to randomly remove some neurons during training and avoid overfitting, the resulting dropout mask can be applied to a layer's activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.853632Z",
     "start_time": "2020-04-02T14:26:53.849944Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      "[1. 1. 1. 0. 1. 1. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(dropout_mask(10, 0.2))\n",
    "print(dropout_mask(10, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the `forward_pass` function that passes the input x through the network up to the output layer for computing the probability for each class using the weight matrices in `W`. The ReLU activation function should be applied on each hidden layer. \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `dropout_rate`: the dropout rate that is used to generate a random dropout mask vector applied after each hidden layer for regularisation.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `out_vals`: a dictionary of output values from each layer: h (the vector before the activation function), a (the resulting vector after passing h from the activation function), its dropout mask vector; and the prediction vector (probability for each class) from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the '1' index array of label data to a '0' index array to make the data efficient for the neural network\n",
    "train_class = train_class -1 \n",
    "dev_class = dev_class -1 \n",
    "test_class = test_class -1\n",
    "\n",
    "Y_tr = train_class\n",
    "Y_dev = dev_class\n",
    "Y_te = test_class\n",
    "X_te = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:54.761268Z",
     "start_time": "2020-04-02T14:26:54.753402Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "    #print(len(x))\n",
    "    \n",
    "    out_vals = {} # Dictionary of h and a \n",
    "    h_vecs = [] # h list\n",
    "    a_vecs = [] # a vector passing h\n",
    "    dropout_vecs = [] # dropout mask for normalization\n",
    "    W_length = len(W)-1 \n",
    "    \n",
    "    input_weight = []\n",
    "    length_input = len(x)\n",
    "    \n",
    "    # Calculate the weights for the input layer\n",
    "    for i in x:\n",
    "        input_weight.append(W[0][i])\n",
    "    \n",
    "    # Compute the first h value\n",
    "    h = np.sum(input_weight,axis=0)\n",
    "    h = h /length_input\n",
    "    # Apply ReLU activation function to get a\n",
    "    a = relu(h)\n",
    "    # Apply dropout regularization to a\n",
    "    d = dropout_mask(len(a),dropout_rate)\n",
    "    output = a * d\n",
    "    \n",
    "    # Add the h and a values to their corresponding vectors\n",
    "    h_vecs.append(h)\n",
    "    a_vecs.append(a)\n",
    "    dropout_vecs.append(d)  \n",
    "          \n",
    "    # For layer k = 1,2,3...\n",
    "    for k in range(1, W_length):\n",
    "        # Compute the next h value using the previous output\n",
    "        h = np.dot(output,W[k])  \n",
    "        # Apply ReLU activation function to get a\n",
    "        a = relu(h)\n",
    "        # Apply dropout regularization to a\n",
    "        d = dropout_mask(len(a) ,dropout_rate)\n",
    "        output = a*d \n",
    "        \n",
    "        # Add the h and a values to their corresponding vectors\n",
    "        h_vecs.append(h)\n",
    "        a_vecs.append(a)\n",
    "        dropout_vecs.append(d)\n",
    "    \n",
    "    # Compute the predicted y value using the final output and weights\n",
    "    y_array = softmax(np.dot(output,W[W_length]))\n",
    "    \n",
    "    # Assign the calculated values to the dictionary\n",
    "    out_vals['h'] = h_vecs\n",
    "    out_vals['a'] = a_vecs\n",
    "    out_vals['dropout_vecs'] = dropout_vecs\n",
    "    out_vals['y'] = y_array\n",
    "    \n",
    "    # Return the dictionary containing h and a values for all layers, dropout masks and the predicted y value\n",
    "    return out_vals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward_pass\n",
    "\n",
    "This function performs a forward pass through the network given the input 'x' and the network weight 'W'.\n",
    "\n",
    "The input x is an array of integers that represents a vocabulary's word indexes. Each weight matrix in the dictionary of network weights (W) connects two layers of the network. \n",
    "\n",
    "The function begins by initialising empty lists to hold the dropout masks for regularisation as well as the h and a vectors for each layer. The function then computes the weighted sum of the input features for the input layer and applies the ReLU activation function to it. To avoid overfitting, a dropout mask is applied to the ensuing activations. The generated h and a vectors are kept in the appropriate lists.\n",
    "\n",
    "The function then repeats the identical calculations for each further layer, taking the weighted sum of the activations from the preceding layer, running it through the ReLU activation function, applying dropout, and storing the resulting h and a vectors.\n",
    "\n",
    "The function then applies a softmax function to the dot product of the weights and activations of the last layer to determine the output of the network, returning the resulting prediction y.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30845276 0.35704102 0.33450622]\n"
     ]
    }
   ],
   "source": [
    "out_vals = forward_pass([1743, 1754, 1225, 593, 1915, 505, 1453, 1262, 1474, 1674, 621, 89], W, dropout_rate=0.2)\n",
    "print(out_vals['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward_pass` function computes the gradients and updates the weights for each matrix in the network from the output to the input. It takes as input \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `y`: the true label\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `out_vals`: a dictionary of output values from a forward pass.\n",
    "- `learning_rate`: the learning rate for updating the weights.\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: the updated weights of the network.\n",
    "\n",
    "Hint: the gradients on the output layer are similar to the multiclass logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T08:24:13.732705Z",
     "start_time": "2020-05-11T08:24:13.729741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.01, freeze_emb=False):\n",
    "    \n",
    "    # Initialize gradients\n",
    "    dW = [np.zeros_like(w) for w in W]\n",
    "    \n",
    "    # Compute error at output layer\n",
    "    error = out_vals['y']\n",
    "    error[y] -= 1\n",
    "    \n",
    "    # Backpropagate error through output layer\n",
    "    dW[-1] = np.outer(out_vals['a'][-1], error)\n",
    "    delta = error\n",
    "    \n",
    "    # Backpropagate error through hidden layers\n",
    "    for i in range(len(W) - 2, 0, -1):\n",
    "        # Retrieve dropout mask for current layer\n",
    "        dropout = out_vals['dropout_vecs'][i-1]\n",
    "        # Compute delta for current layer\n",
    "        delta = np.dot(W[i+1], delta) * relu_derivative(out_vals['h'][i])\n",
    "        delta *= dropout\n",
    "        # Compute gradients for current layer\n",
    "        dW[i] = np.outer(out_vals['a'][i], delta)\n",
    "    \n",
    "    # Backpropagate error through input layer\n",
    "    delta = np.dot(W[1], delta) * relu_derivative(out_vals['h'][0])\n",
    "    dW[0] = np.zeros_like(W[0])\n",
    "    for i in x:\n",
    "        dW[0][i,:] += delta\n",
    "    if freeze_emb:\n",
    "        dW[0] = np.zeros_like(W[0])\n",
    "    \n",
    "    # Update weights\n",
    "    for i in range(len(W)):\n",
    "        W[i] -= lr * dW[i]\n",
    "    \n",
    "    return W\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward_pass\n",
    "\n",
    "This function performs the backward pass for our neural network by using stochastic gradient descent\n",
    "\n",
    "The gradients for each weight matrix are first initialised to zero by the function. The difference between the actual output and the output predicted by the algorithm is then used to calculate the error at the output layer. After determining the delta for each layer using the chain rule, the error is subsequently backpropagated throughout the network. The delta and the associated forward pass activations are then used to calculate the gradients for each layer.\n",
    "\n",
    "Finally, the weights are updated using the gradients and a learning rate. In the event that freeze_emb is set to True, the gradients for the input layer—which houses the embeddings—are set to zero, thus freezing the embeddings during training.\n",
    "\n",
    "The function returns the updated weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[ 0.19698426, -0.21242292, -0.27402183, ...,  0.3449887 ,\n",
       "         -0.376077  ,  0.0964869 ],\n",
       "        [-0.48326415,  0.22214288, -0.49284467, ...,  0.0487023 ,\n",
       "         -0.47245708, -0.468082  ],\n",
       "        [ 0.20135981,  0.20758112,  0.45993912, ...,  0.34579822,\n",
       "         -0.11632725, -0.43926036],\n",
       "        ...,\n",
       "        [ 0.08108832, -0.04608797, -0.01016513, ..., -0.481566  ,\n",
       "         -0.20555991,  0.44186756],\n",
       "        [ 0.19522132, -0.02899734,  0.03076438, ...,  0.4761879 ,\n",
       "          0.22056226, -0.02757094],\n",
       "        [ 0.02454556, -0.13820393, -0.40596458, ..., -0.17762911,\n",
       "         -0.24093711,  0.40163487]], dtype=float32),\n",
       " 1: array([[-0.17466027, -0.06110911, -0.46969202],\n",
       "        [-0.4001671 ,  0.34655026, -0.39759895],\n",
       "        [ 0.3669846 , -0.18752967,  0.17126743],\n",
       "        [-0.10111821, -0.28856072,  0.26094216],\n",
       "        [-0.24831566,  0.19033252, -0.33913198],\n",
       "        [ 0.39314064, -0.43270484,  0.4378048 ],\n",
       "        [-0.42476106, -0.12251305, -0.10697404],\n",
       "        [ 0.10234449,  0.26964295, -0.28170213],\n",
       "        [ 0.28938144,  0.23970783,  0.2799539 ],\n",
       "        [ 0.0072436 , -0.36012924, -0.3735832 ],\n",
       "        [ 0.37037477,  0.48161572, -0.28341383],\n",
       "        [ 0.02518826,  0.3185745 , -0.46291918],\n",
       "        [-0.40202522, -0.3767284 ,  0.35739523],\n",
       "        [ 0.30329818,  0.49644592, -0.32577243],\n",
       "        [-0.21070169, -0.40040782, -0.21175478],\n",
       "        [ 0.46571067, -0.13448916, -0.45205155],\n",
       "        [ 0.27318522, -0.111136  ,  0.39506444],\n",
       "        [-0.3195149 ,  0.20021811,  0.27266335],\n",
       "        [-0.49626544,  0.15025899, -0.01907899],\n",
       "        [ 0.15856373, -0.20289679,  0.354096  ],\n",
       "        [-0.35413313, -0.18214823,  0.33785725],\n",
       "        [ 0.48752478, -0.38862202, -0.2777092 ],\n",
       "        [ 0.0974237 ,  0.05436439,  0.21574757],\n",
       "        [ 0.2355662 ,  0.01056883, -0.28670627],\n",
       "        [-0.0396763 , -0.39464694,  0.16965932],\n",
       "        [-0.40742263,  0.29316887,  0.14802407],\n",
       "        [-0.07471713, -0.03537242, -0.05653787],\n",
       "        [-0.23492356,  0.02102305,  0.23146778],\n",
       "        [ 0.41119906,  0.04621457,  0.36333746],\n",
       "        [ 0.14420246,  0.24073058,  0.18661633],\n",
       "        [ 0.29309848, -0.21135233, -0.0708115 ],\n",
       "        [-0.1603318 ,  0.42817342,  0.15033478],\n",
       "        [-0.4160575 ,  0.2377481 ,  0.10516784],\n",
       "        [-0.4992382 , -0.11251452,  0.06161881],\n",
       "        [-0.3710008 , -0.18750913, -0.38702357],\n",
       "        [ 0.24706374, -0.18099344, -0.21166785],\n",
       "        [-0.39348522, -0.00878966,  0.36888903],\n",
       "        [-0.4815273 , -0.08536544,  0.25213778],\n",
       "        [ 0.44136164,  0.45993325,  0.03844387],\n",
       "        [ 0.01067059,  0.2521008 , -0.12589766],\n",
       "        [-0.16942534, -0.10565612, -0.44149926],\n",
       "        [-0.38642406,  0.3884843 ,  0.25409856],\n",
       "        [-0.00931551, -0.09440558,  0.3618336 ],\n",
       "        [ 0.4145899 ,  0.03187022, -0.46191007],\n",
       "        [ 0.3249844 ,  0.10067846,  0.25446582],\n",
       "        [ 0.28902715, -0.2886611 , -0.36081597],\n",
       "        [ 0.08821215,  0.32209778,  0.41982004],\n",
       "        [-0.13038073,  0.3201865 ,  0.32498777],\n",
       "        [-0.2611297 , -0.19860202,  0.37714106],\n",
       "        [ 0.33098915,  0.43323204,  0.1654242 ],\n",
       "        [ 0.48592404, -0.29741865,  0.17658325],\n",
       "        [ 0.25706667,  0.33545104,  0.139119  ],\n",
       "        [ 0.27869648,  0.13141823, -0.34718132],\n",
       "        [-0.27718356, -0.47443604, -0.3943882 ],\n",
       "        [-0.02865371,  0.49969932, -0.24098054],\n",
       "        [ 0.08880151,  0.14796779,  0.43779695],\n",
       "        [-0.0958064 ,  0.44660142,  0.3362052 ],\n",
       "        [ 0.26609176, -0.2659589 , -0.41276357],\n",
       "        [ 0.09547713,  0.34537286,  0.36009872],\n",
       "        [-0.00215778,  0.28117448, -0.07026198],\n",
       "        [-0.22000904,  0.02587325,  0.35910073],\n",
       "        [ 0.38611057, -0.43900338,  0.09103321],\n",
       "        [ 0.3085831 , -0.1982248 , -0.27877718],\n",
       "        [ 0.43828818,  0.12003765,  0.30499244],\n",
       "        [ 0.03850316,  0.09383225, -0.0158395 ],\n",
       "        [-0.1206841 , -0.10888293, -0.07919695],\n",
       "        [-0.02934076,  0.25538492,  0.46677437],\n",
       "        [-0.16609454, -0.00385533, -0.07349507],\n",
       "        [-0.42233822, -0.26007393, -0.4736816 ],\n",
       "        [ 0.4393391 , -0.30684105,  0.12384166],\n",
       "        [-0.02161598, -0.26553202, -0.10643223],\n",
       "        [-0.13571203,  0.37723413, -0.06379702],\n",
       "        [-0.32272676,  0.37805605,  0.02740451],\n",
       "        [-0.01761202, -0.0599079 , -0.2232495 ],\n",
       "        [ 0.30389607,  0.2807669 , -0.34166533],\n",
       "        [-0.1529916 , -0.12249786,  0.01340461],\n",
       "        [-0.43063724,  0.04717811,  0.09394048],\n",
       "        [ 0.12343593,  0.21700485, -0.393481  ],\n",
       "        [ 0.05656289,  0.35245806,  0.39825556],\n",
       "        [-0.45093182, -0.38870448,  0.22048236],\n",
       "        [ 0.4925275 , -0.08096074, -0.37476537],\n",
       "        [ 0.10524027, -0.07410254,  0.06966387],\n",
       "        [ 0.15819988,  0.05339287, -0.32764348],\n",
       "        [-0.17082171,  0.38185248,  0.21836518],\n",
       "        [ 0.34886658, -0.4040761 , -0.36642644],\n",
       "        [ 0.37415397, -0.08265737, -0.23061152],\n",
       "        [ 0.32481855, -0.3298984 , -0.27862948],\n",
       "        [ 0.24926405, -0.07785861, -0.49379727],\n",
       "        [-0.130763  ,  0.4497752 , -0.44175935],\n",
       "        [-0.07877092, -0.19199923,  0.38563105],\n",
       "        [-0.00785201,  0.41227818,  0.04086424],\n",
       "        [ 0.3463292 , -0.2388646 , -0.09131172],\n",
       "        [ 0.22757177,  0.3536308 , -0.19173   ],\n",
       "        [-0.12891205,  0.12490982,  0.11821734],\n",
       "        [ 0.2633477 ,  0.166286  ,  0.10571574],\n",
       "        [ 0.21790981, -0.215271  ,  0.4127618 ],\n",
       "        [-0.37342227, -0.17290477,  0.164614  ],\n",
       "        [-0.13258341, -0.1877448 , -0.3060575 ],\n",
       "        [ 0.04296459, -0.30829233,  0.04662165],\n",
       "        [-0.27796403, -0.02952603,  0.3453988 ],\n",
       "        [-0.06247269,  0.23000704, -0.191093  ],\n",
       "        [-0.29014263,  0.1171051 ,  0.48841327],\n",
       "        [-0.11100549, -0.4905545 ,  0.11374731],\n",
       "        [-0.4675246 ,  0.2565275 ,  0.12907231],\n",
       "        [ 0.33332857, -0.11343259,  0.3474142 ],\n",
       "        [ 0.29147154,  0.43222737, -0.04930749],\n",
       "        [ 0.49821988, -0.15055576, -0.07678368],\n",
       "        [ 0.06682598,  0.15504515,  0.41462427],\n",
       "        [-0.06367322, -0.41506177, -0.210994  ],\n",
       "        [ 0.12493771,  0.3034379 , -0.38205844],\n",
       "        [-0.01647695, -0.37015727,  0.26487005],\n",
       "        [-0.1063042 ,  0.4938643 ,  0.304189  ],\n",
       "        [ 0.2908457 , -0.14892283, -0.17705646],\n",
       "        [ 0.25438368,  0.03438495, -0.43557128],\n",
       "        [-0.08567578,  0.16302535,  0.37682703],\n",
       "        [-0.26355714,  0.14540438, -0.40385056],\n",
       "        [-0.3539514 ,  0.4886267 , -0.37122348],\n",
       "        [ 0.05633373, -0.18465838,  0.12011731],\n",
       "        [-0.15716855, -0.27891138,  0.07624409],\n",
       "        [-0.3553232 , -0.13530865, -0.49244255],\n",
       "        [-0.454751  ,  0.0357024 , -0.3870413 ],\n",
       "        [-0.012756  ,  0.44666916,  0.495565  ],\n",
       "        [-0.49828663, -0.31512722,  0.2747573 ],\n",
       "        [ 0.45943558, -0.1278682 ,  0.30009508],\n",
       "        [-0.05493442, -0.47331634,  0.05152392],\n",
       "        [ 0.12869632, -0.27147108,  0.18795629],\n",
       "        [-0.33047146,  0.47524947,  0.15041095],\n",
       "        [-0.24650283, -0.11748676,  0.37598303],\n",
       "        [-0.09278218,  0.03607221, -0.06087457],\n",
       "        [-0.45417556,  0.22189732,  0.26314923],\n",
       "        [-0.3285758 ,  0.20227821, -0.47299388],\n",
       "        [-0.11971237,  0.11165971,  0.02040183],\n",
       "        [-0.4483136 , -0.35738835,  0.3259519 ],\n",
       "        [-0.05156212, -0.1934062 , -0.22319962],\n",
       "        [-0.24195938, -0.47238675,  0.46143046],\n",
       "        [ 0.023168  , -0.45248502, -0.43140703],\n",
       "        [ 0.14223053,  0.47381666,  0.41372678],\n",
       "        [-0.14915732, -0.00770148,  0.17318782],\n",
       "        [ 0.42349112, -0.06588558,  0.43850434],\n",
       "        [-0.06109855,  0.47741726, -0.43241528],\n",
       "        [-0.2865871 ,  0.4057888 ,  0.4715665 ],\n",
       "        [ 0.32480764,  0.39055943, -0.35924166],\n",
       "        [-0.367449  , -0.04570685,  0.38271993],\n",
       "        [-0.31265938, -0.00402672, -0.32716945],\n",
       "        [ 0.4906239 , -0.4442139 , -0.1718426 ],\n",
       "        [-0.43703806,  0.06963243,  0.17777592],\n",
       "        [-0.22296904, -0.25758678,  0.24522816],\n",
       "        [ 0.35074794,  0.48358357,  0.43014956],\n",
       "        [-0.15822946, -0.1474657 ,  0.38718936],\n",
       "        [ 0.24864046,  0.1921876 , -0.09640855],\n",
       "        [ 0.2145137 , -0.47129026,  0.3784697 ],\n",
       "        [ 0.44600433, -0.14823931,  0.07459683],\n",
       "        [ 0.43701288, -0.11489751,  0.43774614],\n",
       "        [ 0.1883965 , -0.09931582,  0.10977363],\n",
       "        [-0.36733335,  0.47684386, -0.35068005],\n",
       "        [-0.32703656, -0.49673665,  0.48770648],\n",
       "        [-0.08098656, -0.14881256, -0.04988773],\n",
       "        [ 0.09739836,  0.2306179 , -0.22092988],\n",
       "        [ 0.06451932, -0.4617801 ,  0.24251927],\n",
       "        [ 0.3277334 ,  0.09523882,  0.44358978],\n",
       "        [-0.47162452, -0.44061357, -0.11219527],\n",
       "        [ 0.38906428,  0.22835425, -0.40859506],\n",
       "        [-0.28392646,  0.0526468 ,  0.2570443 ],\n",
       "        [-0.11532158, -0.08587721,  0.04888657],\n",
       "        [ 0.26166114, -0.49540132,  0.32698786],\n",
       "        [-0.48201773, -0.20266543,  0.10847956],\n",
       "        [-0.12606731,  0.05816259, -0.3380912 ],\n",
       "        [ 0.40022734, -0.07630995,  0.25375742],\n",
       "        [ 0.2820957 , -0.01045185,  0.2652224 ],\n",
       "        [-0.079115  ,  0.13680328, -0.36749995],\n",
       "        [-0.01792427, -0.17626138,  0.42160058],\n",
       "        [-0.313239  , -0.42600688, -0.27595448],\n",
       "        [-0.2712783 , -0.33659938,  0.03780233],\n",
       "        [-0.07744171, -0.2340265 , -0.05354127],\n",
       "        [ 0.14537784,  0.15915674,  0.16494274],\n",
       "        [-0.39004475, -0.06745032, -0.17807393],\n",
       "        [ 0.3412176 , -0.30838346,  0.20452608],\n",
       "        [-0.03032689,  0.21516435,  0.09714964],\n",
       "        [-0.13654387, -0.2871967 ,  0.24710183],\n",
       "        [-0.3062164 ,  0.00065071, -0.43810803],\n",
       "        [-0.4053547 ,  0.18866466,  0.08946785],\n",
       "        [-0.09638367,  0.33505666, -0.08177908],\n",
       "        [-0.00579377,  0.17535563, -0.48971623],\n",
       "        [-0.20722641,  0.05240709,  0.30519935],\n",
       "        [-0.35406458,  0.45569688, -0.2002567 ],\n",
       "        [ 0.17940539,  0.08866059,  0.3718703 ],\n",
       "        [ 0.00872873,  0.30035573, -0.0535555 ],\n",
       "        [-0.30973014, -0.13704534, -0.33540183],\n",
       "        [ 0.4529601 , -0.29800934,  0.04096342],\n",
       "        [-0.3831516 , -0.07468665,  0.24125572],\n",
       "        [-0.22439118, -0.28980976,  0.29350746],\n",
       "        [ 0.12949319,  0.08348865, -0.19384569],\n",
       "        [-0.28083435, -0.44213894, -0.07923295],\n",
       "        [ 0.38787323,  0.36326393, -0.06870244],\n",
       "        [-0.498412  ,  0.23340942, -0.01893093],\n",
       "        [-0.02353635, -0.4307205 ,  0.2641254 ],\n",
       "        [ 0.20387273, -0.03915742, -0.17727779],\n",
       "        [-0.03068743, -0.4148114 ,  0.2320792 ],\n",
       "        [-0.20048846,  0.35012794, -0.39284056],\n",
       "        [-0.09244375,  0.49618816, -0.03383967],\n",
       "        [ 0.31190905,  0.2951092 , -0.12648286],\n",
       "        [-0.08838225, -0.45286217, -0.49231094],\n",
       "        [-0.162696  ,  0.44990653, -0.0400196 ],\n",
       "        [-0.37911218, -0.27656838, -0.10620528],\n",
       "        [-0.4443068 ,  0.4902025 ,  0.48088753],\n",
       "        [ 0.22013177,  0.4038213 , -0.06646896],\n",
       "        [-0.20470743, -0.3262583 ,  0.12584455],\n",
       "        [-0.1014336 ,  0.26144707,  0.33301854],\n",
       "        [-0.30433178,  0.08084474,  0.22455303],\n",
       "        [-0.04781009,  0.41899222,  0.33168328],\n",
       "        [-0.23543121, -0.26936582,  0.2871682 ],\n",
       "        [-0.08124934,  0.21573891, -0.35962185],\n",
       "        [-0.4425267 ,  0.30503646, -0.40129417],\n",
       "        [ 0.40410966,  0.32913744,  0.14041743],\n",
       "        [-0.07032523, -0.4991006 ,  0.44458282],\n",
       "        [ 0.31498578, -0.17299856, -0.42037556],\n",
       "        [ 0.23149258,  0.25897333, -0.15391049],\n",
       "        [ 0.12056574, -0.19620237, -0.32967484],\n",
       "        [ 0.4914293 ,  0.31458923, -0.35828748],\n",
       "        [ 0.3288428 , -0.19511484,  0.25935715],\n",
       "        [ 0.48545283,  0.2343792 , -0.3639739 ],\n",
       "        [ 0.4788201 ,  0.28091162,  0.4163697 ],\n",
       "        [-0.0121935 , -0.4108255 ,  0.26175827],\n",
       "        [-0.44168442,  0.12280365,  0.2203171 ],\n",
       "        [ 0.09354157, -0.0883142 ,  0.36839768],\n",
       "        [-0.4423439 , -0.39157805, -0.19408001],\n",
       "        [ 0.25541583,  0.03283333, -0.22083549],\n",
       "        [-0.25435355,  0.08148849, -0.2803547 ],\n",
       "        [ 0.36486724, -0.32265246, -0.27783543],\n",
       "        [-0.2917027 , -0.43944812, -0.31282413],\n",
       "        [-0.0149707 ,  0.14516777,  0.46555293],\n",
       "        [-0.42774692, -0.3066772 ,  0.17748961],\n",
       "        [-0.3556774 , -0.2633445 , -0.00129752],\n",
       "        [ 0.45738107,  0.18752189, -0.28958532],\n",
       "        [ 0.3887255 , -0.21139427,  0.36324763],\n",
       "        [-0.44129542,  0.08425905, -0.14337368],\n",
       "        [ 0.2972725 , -0.27104002,  0.24891339],\n",
       "        [-0.17977645, -0.474699  , -0.2586271 ],\n",
       "        [ 0.2022727 ,  0.07906845, -0.37426972],\n",
       "        [ 0.02457739,  0.3771407 ,  0.09176014],\n",
       "        [-0.43661588,  0.4913721 , -0.02188712],\n",
       "        [ 0.2781207 , -0.11035361, -0.38666844],\n",
       "        [ 0.1730765 , -0.2463676 , -0.22989021],\n",
       "        [-0.28605402, -0.4937272 ,  0.1403093 ],\n",
       "        [ 0.08775395, -0.14472803,  0.37862968],\n",
       "        [-0.24963982, -0.31707036, -0.03850818],\n",
       "        [-0.30596995,  0.38761652,  0.14924097],\n",
       "        [ 0.21526693, -0.3203269 ,  0.18845157],\n",
       "        [-0.08741924,  0.06399693,  0.25326896],\n",
       "        [ 0.3935925 , -0.29824755, -0.36078927],\n",
       "        [ 0.13355629,  0.24166715,  0.28408659],\n",
       "        [ 0.05313417, -0.09635743,  0.3647878 ],\n",
       "        [-0.3114536 ,  0.16135763, -0.04337631],\n",
       "        [-0.40593788,  0.10193351, -0.21512523],\n",
       "        [-0.11741855, -0.2666094 , -0.15461484],\n",
       "        [ 0.1845554 ,  0.4163492 ,  0.1396141 ],\n",
       "        [ 0.38386348, -0.011571  ,  0.38378042],\n",
       "        [-0.28917333, -0.35325146, -0.12815812],\n",
       "        [ 0.24801621, -0.37411964,  0.20472741],\n",
       "        [-0.26496068,  0.376605  , -0.25568867],\n",
       "        [ 0.39741752, -0.242882  ,  0.1595755 ],\n",
       "        [ 0.24663372,  0.44406196, -0.25066465],\n",
       "        [-0.35878244,  0.31307286,  0.0572404 ],\n",
       "        [ 0.2331764 ,  0.31194153,  0.19940181],\n",
       "        [ 0.44780535,  0.14314531, -0.32466242],\n",
       "        [-0.02352688,  0.02517445, -0.42420742],\n",
       "        [-0.4289797 , -0.09864564,  0.45403937],\n",
       "        [-0.31873915, -0.4552216 ,  0.17362931],\n",
       "        [ 0.39544022, -0.32480586, -0.45540932],\n",
       "        [-0.1829683 ,  0.12779066,  0.18906203],\n",
       "        [-0.25240746, -0.01230146, -0.11354056],\n",
       "        [-0.42880797, -0.3172408 ,  0.0567129 ],\n",
       "        [ 0.4387505 , -0.27121425,  0.2569611 ],\n",
       "        [-0.2519418 ,  0.463916  ,  0.2939323 ],\n",
       "        [ 0.30384788,  0.44739038, -0.4542912 ],\n",
       "        [-0.3611295 , -0.2992809 , -0.327845  ],\n",
       "        [-0.13151576,  0.2551657 , -0.37077892],\n",
       "        [ 0.3496126 ,  0.4613393 ,  0.10135645],\n",
       "        [ 0.26207304,  0.28226843, -0.05791699],\n",
       "        [-0.07890894,  0.35515624,  0.17357592],\n",
       "        [ 0.40822002,  0.21956849, -0.4210895 ],\n",
       "        [-0.09109077, -0.13097598,  0.07225776],\n",
       "        [ 0.09099241,  0.00864978,  0.21728566],\n",
       "        [ 0.28569826,  0.30894253,  0.10170867],\n",
       "        [ 0.271871  , -0.01331332,  0.200761  ],\n",
       "        [-0.45529673,  0.24145861,  0.3741751 ],\n",
       "        [ 0.36680332, -0.15388103,  0.36910826],\n",
       "        [-0.29803988,  0.04173325, -0.4190061 ],\n",
       "        [-0.4372683 , -0.06344825, -0.02334288],\n",
       "        [-0.17419012, -0.15470353,  0.13074529],\n",
       "        [-0.24080683,  0.33952057, -0.4474191 ],\n",
       "        [-0.02457705,  0.31718585, -0.49177974],\n",
       "        [-0.21735607,  0.06481802,  0.09549817],\n",
       "        [-0.17849779,  0.20275275, -0.06580228],\n",
       "        [-0.47140944,  0.27176473,  0.13455532],\n",
       "        [ 0.09232859,  0.14278053,  0.11672509],\n",
       "        [-0.19561265, -0.35213792,  0.04952338],\n",
       "        [ 0.314044  , -0.16219382,  0.46105266],\n",
       "        [-0.46005508,  0.02299332,  0.18673874],\n",
       "        [ 0.16002364, -0.44106308, -0.16392617]], dtype=float32)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BP= backward_pass([0,0,0,1,1], 1, W, out_vals, lr=0.001, freeze_emb=False)\n",
    "BP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support back-propagation by using the `forward_pass` and `backward_pass` functions.\n",
    "\n",
    "The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `W`: the weights of the network (dictionary)\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `dropout`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated (to be used by the backward pass function).\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:19.021428Z",
     "start_time": "2020-04-02T15:09:19.017835Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.001, dropout=0.2, epochs=5, \n",
    "        tolerance=0.001, freeze_emb=False, print_progress=True):\n",
    "  \n",
    "    # Initialize loss history and best weights\n",
    "    # Initialize loss history and best weights\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    prev_validation_loss = float(\"inf\")\n",
    "    best_weights = None\n",
    "\n",
    "    # Train for the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        # Shuffle training data\n",
    "        p = np.random.permutation(len(X_tr)).astype(np.int32)\n",
    "        X_tr = [X_tr[i] for i in p]\n",
    "        Y_tr = [Y_tr[i] for i in p]\n",
    "    \n",
    "        # Train on each example\n",
    "        for i in range(len(X_tr)):\n",
    "            x = X_tr[i]\n",
    "            y = Y_tr[i]\n",
    "        \n",
    "            # Forward pass\n",
    "            out_vals = forward_pass(x, W, dropout_rate=dropout)\n",
    "        \n",
    "            # Compute loss and gradients using backward pass\n",
    "            W = backward_pass(x, y, W, out_vals, lr=lr, freeze_emb=freeze_emb)\n",
    "    \n",
    "        # Compute and store training loss\n",
    "        training_loss = sum(categorical_loss(Y_tr[i], forward_pass(X_tr[i], W, dropout_rate=0)['y']) \n",
    "                    for i in range(len(X_tr))) / len(X_tr)\n",
    "        training_loss_history.append(training_loss)\n",
    "    \n",
    "        # Compute and store validation loss\n",
    "        validation_loss = sum(categorical_loss(Y_dev[i], forward_pass(X_dev[i], W, dropout_rate=0)['y']) \n",
    "                  for i in range(len(X_dev))) / len(X_dev)\n",
    "        validation_loss_history.append(validation_loss)\n",
    "    \n",
    "        # Check if validation loss has increased else stop the training early \n",
    "        if validation_loss < prev_validation_loss:\n",
    "            prev_validation_loss = validation_loss\n",
    "            best_weights = W\n",
    "        else:\n",
    "            if validation_loss >= prev_validation_loss - tolerance:\n",
    "                if print_progress:\n",
    "                    print(f\"Epoch {epoch+1} stopped early: train loss {training_loss:.4f}, val loss {validation_loss:.4f}\")\n",
    "                return best_weights, training_loss_history, validation_loss_history\n",
    "        \n",
    "        # Print progress\n",
    "        if print_progress:\n",
    "            print(f\"Epoch {epoch+1}: train loss {training_loss:.4f}, val loss {validation_loss:.4f}\")\n",
    "\n",
    "    # Return the best weights and loss history\n",
    "    return best_weights, training_loss_history, validation_loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD function\n",
    "\n",
    "The function trains the model iteratively over a predetermined number of epochs, training the model on each example in the dataset while randomly rearranging the training data. The function uses the computed gradients with respect to the loss function to update the weights at the end of each epoch.\n",
    "\n",
    "It also keeps track of the loss values during training and validation, and stores them in the training_loss_history and validation_loss_history lists, respectively.The function also stores the best weights depending on the least amount of validation loss that has been seen thus far.\n",
    "\n",
    "The function returns the best weights found during training, as well as the training and validation loss histories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate your neural net. First, you need to define your network using the `network_weights` function followed by SGD with backprop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:15.716497Z",
     "start_time": "2020-04-02T14:27:15.612736Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.0180, val loss 1.0700\n",
      "Epoch 2: train loss 0.9411, val loss 1.0165\n",
      "Epoch 3: train loss 0.8738, val loss 0.9681\n",
      "Epoch 4: train loss 0.8139, val loss 0.9233\n",
      "Epoch 5: train loss 0.7601, val loss 0.8814\n",
      "Epoch 6: train loss 0.7117, val loss 0.8422\n",
      "Epoch 7: train loss 0.6678, val loss 0.8054\n",
      "Epoch 8: train loss 0.6283, val loss 0.7710\n",
      "Epoch 9: train loss 0.5922, val loss 0.7390\n",
      "Epoch 10: train loss 0.5596, val loss 0.7091\n",
      "Epoch 11: train loss 0.5300, val loss 0.6815\n",
      "Epoch 12: train loss 0.5031, val loss 0.6558\n",
      "Epoch 13: train loss 0.4786, val loss 0.6315\n",
      "Epoch 14: train loss 0.4561, val loss 0.6093\n",
      "Epoch 15: train loss 0.4356, val loss 0.5888\n",
      "Epoch 16: train loss 0.4167, val loss 0.5698\n",
      "Epoch 17: train loss 0.3995, val loss 0.5525\n",
      "Epoch 18: train loss 0.3837, val loss 0.5357\n",
      "Epoch 19: train loss 0.3689, val loss 0.5209\n",
      "Epoch 20: train loss 0.3552, val loss 0.5064\n",
      "Epoch 21: train loss 0.3423, val loss 0.4934\n",
      "Epoch 22: train loss 0.3304, val loss 0.4808\n",
      "Epoch 23: train loss 0.3192, val loss 0.4692\n",
      "Epoch 24: train loss 0.3087, val loss 0.4589\n",
      "Epoch 25: train loss 0.2988, val loss 0.4492\n",
      "Epoch 26: train loss 0.2894, val loss 0.4396\n",
      "Epoch 27: train loss 0.2805, val loss 0.4309\n",
      "Epoch 28: train loss 0.2721, val loss 0.4230\n",
      "Epoch 29: train loss 0.2641, val loss 0.4150\n",
      "Epoch 30: train loss 0.2565, val loss 0.4073\n",
      "Epoch 31: train loss 0.2493, val loss 0.4006\n",
      "Epoch 32: train loss 0.2425, val loss 0.3939\n",
      "Epoch 33: train loss 0.2358, val loss 0.3884\n",
      "Epoch 34: train loss 0.2294, val loss 0.3831\n",
      "Epoch 35: train loss 0.2233, val loss 0.3779\n",
      "Epoch 36: train loss 0.2175, val loss 0.3734\n",
      "Epoch 37: train loss 0.2119, val loss 0.3686\n",
      "Epoch 38: train loss 0.2065, val loss 0.3643\n",
      "Epoch 39: train loss 0.2013, val loss 0.3604\n",
      "Epoch 40: train loss 0.1964, val loss 0.3566\n",
      "Epoch 41: train loss 0.1915, val loss 0.3533\n",
      "Epoch 42: train loss 0.1869, val loss 0.3500\n",
      "Epoch 43: train loss 0.1824, val loss 0.3469\n",
      "Epoch 44: train loss 0.1780, val loss 0.3440\n",
      "Epoch 45: train loss 0.1739, val loss 0.3407\n",
      "Epoch 46: train loss 0.1699, val loss 0.3377\n",
      "Epoch 47: train loss 0.1659, val loss 0.3355\n",
      "Epoch 48: train loss 0.1622, val loss 0.3332\n",
      "Epoch 49: train loss 0.1585, val loss 0.3311\n",
      "Epoch 50: train loss 0.1550, val loss 0.3287\n",
      "Epoch 51: train loss 0.1515, val loss 0.3267\n",
      "Epoch 52: train loss 0.1481, val loss 0.3251\n",
      "Epoch 53: train loss 0.1449, val loss 0.3230\n",
      "Epoch 54: train loss 0.1418, val loss 0.3211\n",
      "Epoch 55: train loss 0.1387, val loss 0.3200\n",
      "Epoch 56: train loss 0.1358, val loss 0.3183\n",
      "Epoch 57: train loss 0.1329, val loss 0.3163\n",
      "Epoch 58: train loss 0.1302, val loss 0.3151\n",
      "Epoch 59: train loss 0.1274, val loss 0.3141\n",
      "Epoch 60: train loss 0.1248, val loss 0.3126\n",
      "Epoch 61: train loss 0.1223, val loss 0.3110\n",
      "Epoch 62: train loss 0.1198, val loss 0.3098\n",
      "Epoch 63: train loss 0.1173, val loss 0.3084\n",
      "Epoch 64: train loss 0.1149, val loss 0.3079\n",
      "Epoch 65: train loss 0.1126, val loss 0.3064\n",
      "Epoch 66: train loss 0.1103, val loss 0.3057\n",
      "Epoch 67: train loss 0.1080, val loss 0.3049\n",
      "Epoch 68: train loss 0.1058, val loss 0.3041\n",
      "Epoch 69: train loss 0.1037, val loss 0.3032\n",
      "Epoch 70: train loss 0.1017, val loss 0.3027\n",
      "Epoch 71: train loss 0.0997, val loss 0.3024\n",
      "Epoch 72: train loss 0.0978, val loss 0.3015\n",
      "Epoch 73: train loss 0.0959, val loss 0.3007\n",
      "Epoch 74: train loss 0.0941, val loss 0.2999\n",
      "Epoch 75: train loss 0.0923, val loss 0.2993\n",
      "Epoch 76: train loss 0.0906, val loss 0.2986\n",
      "Epoch 77: train loss 0.0888, val loss 0.2982\n",
      "Epoch 78: train loss 0.0872, val loss 0.2972\n",
      "Epoch 79: train loss 0.0856, val loss 0.2971\n",
      "Epoch 80: train loss 0.0840, val loss 0.2969\n",
      "Epoch 81: train loss 0.0824, val loss 0.2966\n",
      "Epoch 82: train loss 0.0809, val loss 0.2961\n",
      "Epoch 83: train loss 0.0795, val loss 0.2957\n",
      "Epoch 84: train loss 0.0781, val loss 0.2952\n",
      "Epoch 85: train loss 0.0767, val loss 0.2944\n",
      "Epoch 86: train loss 0.0753, val loss 0.2942\n",
      "Epoch 87: train loss 0.0739, val loss 0.2940\n",
      "Epoch 88: train loss 0.0726, val loss 0.2940\n",
      "Epoch 89: train loss 0.0712, val loss 0.2936\n",
      "Epoch 90: train loss 0.0699, val loss 0.2935\n",
      "Epoch 91: train loss 0.0687, val loss 0.2933\n",
      "Epoch 92 stopped early: train loss 0.0675, val loss 0.2934\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(vocab),\n",
    "                    embedding_dim = 300,\n",
    "                    hidden_dim=[],\n",
    "                    num_classes=3)\n",
    "\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(X_tr, Y_tr,\n",
    "                            W,\n",
    "                            X_dev=X_dev, \n",
    "                            Y_dev=Y_dev,\n",
    "                            lr=0.001, \n",
    "                            dropout=0.2,\n",
    "                            freeze_emb=False,\n",
    "                            tolerance=0.001,\n",
    "                            epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the Loss history \n",
    "\n",
    "plt.plot(loss_tr, label='Training loss')\n",
    "plt.plot(dev_loss, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss History')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:10:11.037495Z",
     "start_time": "2020-04-02T15:10:11.034999Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8487208008898777\n",
      "Precision: 0.8512270784089923\n",
      "Recall: 0.848695652173913\n",
      "F1-Score: 0.848831898522301\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y'])\n",
    "            for x,y in zip(X_te,Y_te)]\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.6072, val loss 0.7376\n",
      "Epoch 2: train loss 0.3633, val loss 0.4942\n",
      "Epoch 3: train loss 0.2640, val loss 0.3666\n",
      "Epoch 4: train loss 0.2045, val loss 0.3162\n",
      "Epoch 5: train loss 0.1586, val loss 0.2946\n",
      "Epoch 6: train loss 0.1288, val loss 0.2805\n",
      "Epoch 7 stopped early: train loss 0.1038, val loss 0.2838\n",
      "Epoch 1: train loss 0.6616, val loss 0.7921\n",
      "Epoch 2: train loss 0.4194, val loss 0.5501\n",
      "Epoch 3: train loss 0.3021, val loss 0.4260\n",
      "Epoch 4: train loss 0.2313, val loss 0.3745\n",
      "Epoch 5: train loss 0.1850, val loss 0.3388\n",
      "Epoch 6: train loss 0.1502, val loss 0.3198\n",
      "Epoch 7: train loss 0.1222, val loss 0.3146\n",
      "Epoch 8: train loss 0.1013, val loss 0.3129\n",
      "Epoch 9: train loss 0.0855, val loss 0.3105\n",
      "Epoch 10: train loss 0.0722, val loss 0.3081\n",
      "Epoch 11 stopped early: train loss 0.0615, val loss 0.3095\n",
      "Epoch 1: train loss 0.6354, val loss 0.7302\n",
      "Epoch 2: train loss 0.4135, val loss 0.5236\n",
      "Epoch 3: train loss 0.3010, val loss 0.4292\n",
      "Epoch 4: train loss 0.2327, val loss 0.3642\n",
      "Epoch 5: train loss 0.1864, val loss 0.3359\n",
      "Epoch 6: train loss 0.1519, val loss 0.3219\n",
      "Epoch 7: train loss 0.1267, val loss 0.3066\n",
      "Epoch 8 stopped early: train loss 0.1065, val loss 0.3082\n",
      "Epoch 1: train loss 0.2094, val loss 0.2725\n",
      "Epoch 2: train loss 0.0827, val loss 0.2556\n",
      "Epoch 3 stopped early: train loss 0.0384, val loss 0.3295\n",
      "Epoch 1: train loss 0.1910, val loss 0.3056\n",
      "Epoch 2: train loss 0.0843, val loss 0.2909\n",
      "Epoch 3 stopped early: train loss 0.0471, val loss 0.3090\n",
      "Epoch 1: train loss 0.2017, val loss 0.3635\n",
      "Epoch 2: train loss 0.0905, val loss 0.3129\n",
      "Epoch 3: train loss 0.0479, val loss 0.2809\n",
      "Epoch 4 stopped early: train loss 0.0350, val loss 0.2868\n",
      "Epoch 1: train loss 0.1149, val loss 0.3063\n",
      "Epoch 2 stopped early: train loss 0.0501, val loss 0.4988\n",
      "Epoch 1: train loss 0.1195, val loss 0.3035\n",
      "Epoch 2 stopped early: train loss 0.0525, val loss 0.3261\n",
      "Epoch 1: train loss 0.1273, val loss 0.3200\n",
      "Epoch 2: train loss 0.0480, val loss 0.3169\n",
      "Epoch 3 stopped early: train loss 0.0282, val loss 0.3453\n",
      "Epoch 1: train loss 0.7740, val loss 0.8721\n",
      "Epoch 2: train loss 0.4691, val loss 0.5759\n",
      "Epoch 3: train loss 0.3223, val loss 0.4142\n",
      "Epoch 4: train loss 0.2500, val loss 0.3482\n",
      "Epoch 5: train loss 0.2006, val loss 0.3237\n",
      "Epoch 6: train loss 0.1622, val loss 0.3221\n",
      "Epoch 7: train loss 0.1332, val loss 0.3173\n",
      "Epoch 8: train loss 0.1145, val loss 0.3142\n",
      "Epoch 9 stopped early: train loss 0.0930, val loss 0.3317\n",
      "Epoch 1: train loss 0.7846, val loss 0.8804\n",
      "Epoch 2: train loss 0.4949, val loss 0.6090\n",
      "Epoch 3: train loss 0.3403, val loss 0.4461\n",
      "Epoch 4: train loss 0.2585, val loss 0.3707\n",
      "Epoch 5: train loss 0.2059, val loss 0.3265\n",
      "Epoch 6: train loss 0.1688, val loss 0.3044\n",
      "Epoch 7: train loss 0.1406, val loss 0.2874\n",
      "Epoch 8: train loss 0.1188, val loss 0.2779\n",
      "Epoch 9 stopped early: train loss 0.0984, val loss 0.2826\n",
      "Epoch 1: train loss 0.7652, val loss 0.8611\n",
      "Epoch 2: train loss 0.4972, val loss 0.6126\n",
      "Epoch 3: train loss 0.3525, val loss 0.4626\n",
      "Epoch 4: train loss 0.2709, val loss 0.3920\n",
      "Epoch 5: train loss 0.2173, val loss 0.3519\n",
      "Epoch 6: train loss 0.1789, val loss 0.3295\n",
      "Epoch 7: train loss 0.1497, val loss 0.3154\n",
      "Epoch 8: train loss 0.1266, val loss 0.3020\n",
      "Epoch 9: train loss 0.1079, val loss 0.2983\n",
      "Epoch 10: train loss 0.0928, val loss 0.2958\n",
      "Epoch 11: train loss 0.0806, val loss 0.2901\n",
      "Epoch 12 stopped early: train loss 0.0701, val loss 0.2929\n",
      "Epoch 1: train loss 0.2351, val loss 0.3064\n",
      "Epoch 2: train loss 0.1228, val loss 0.2851\n",
      "Epoch 3 stopped early: train loss 0.0536, val loss 0.3486\n",
      "Epoch 1: train loss 0.2246, val loss 0.3016\n",
      "Epoch 2 stopped early: train loss 0.0992, val loss 0.3056\n",
      "Epoch 1: train loss 0.2289, val loss 0.3852\n",
      "Epoch 2: train loss 0.1108, val loss 0.2905\n",
      "Epoch 3 stopped early: train loss 0.0594, val loss 0.3073\n",
      "Epoch 1: train loss 0.1533, val loss 0.3381\n",
      "Epoch 2 stopped early: train loss 0.0685, val loss 0.3740\n",
      "Epoch 1: train loss 0.1352, val loss 0.3192\n",
      "Epoch 2 stopped early: train loss 0.0538, val loss 0.3434\n",
      "Epoch 1: train loss 0.1464, val loss 0.2783\n",
      "Epoch 2 stopped early: train loss 0.0613, val loss 0.2922\n",
      "Epoch 1: train loss 0.8773, val loss 0.9494\n",
      "Epoch 2: train loss 0.5534, val loss 0.6447\n",
      "Epoch 3: train loss 0.3621, val loss 0.4007\n",
      "Epoch 4: train loss 0.2805, val loss 0.3090\n",
      "Epoch 5: train loss 0.2274, val loss 0.2811\n",
      "Epoch 6: train loss 0.1976, val loss 0.2556\n",
      "Epoch 7 stopped early: train loss 0.1642, val loss 0.2765\n",
      "Epoch 1: train loss 0.9255, val loss 0.9787\n",
      "Epoch 2: train loss 0.6316, val loss 0.7398\n",
      "Epoch 3: train loss 0.4171, val loss 0.5222\n",
      "Epoch 4: train loss 0.3076, val loss 0.4086\n",
      "Epoch 5: train loss 0.2407, val loss 0.3532\n",
      "Epoch 6: train loss 0.1945, val loss 0.3192\n",
      "Epoch 7: train loss 0.1619, val loss 0.2991\n",
      "Epoch 8: train loss 0.1356, val loss 0.2921\n",
      "Epoch 9: train loss 0.1136, val loss 0.2875\n",
      "Epoch 10: train loss 0.0978, val loss 0.2799\n",
      "Epoch 11 stopped early: train loss 0.0825, val loss 0.2929\n",
      "Epoch 1: train loss 0.8835, val loss 0.9687\n",
      "Epoch 2: train loss 0.5911, val loss 0.7223\n",
      "Epoch 3: train loss 0.4062, val loss 0.5355\n",
      "Epoch 4: train loss 0.3070, val loss 0.4310\n",
      "Epoch 5: train loss 0.2451, val loss 0.3847\n",
      "Epoch 6: train loss 0.2009, val loss 0.3497\n",
      "Epoch 7: train loss 0.1680, val loss 0.3293\n",
      "Epoch 8: train loss 0.1426, val loss 0.3161\n",
      "Epoch 9: train loss 0.1221, val loss 0.3106\n",
      "Epoch 10: train loss 0.1053, val loss 0.3061\n",
      "Epoch 11: train loss 0.0912, val loss 0.3014\n",
      "Epoch 12: train loss 0.0807, val loss 0.2963\n",
      "Epoch 13 stopped early: train loss 0.0706, val loss 0.3119\n",
      "Epoch 1: train loss 0.2486, val loss 0.3031\n",
      "Epoch 2 stopped early: train loss 0.1228, val loss 0.3048\n",
      "Epoch 1: train loss 0.2505, val loss 0.3481\n",
      "Epoch 2: train loss 0.1287, val loss 0.2516\n",
      "Epoch 3 stopped early: train loss 0.0657, val loss 0.3087\n",
      "Epoch 1: train loss 0.2600, val loss 0.3953\n",
      "Epoch 2: train loss 0.1191, val loss 0.2934\n",
      "Epoch 3 stopped early: train loss 0.0674, val loss 0.3083\n",
      "Epoch 1: train loss 0.2053, val loss 0.5341\n",
      "Epoch 2 stopped early: train loss 0.0948, val loss 0.5668\n",
      "Epoch 1: train loss 0.1548, val loss 0.2706\n",
      "Epoch 2 stopped early: train loss 0.0595, val loss 0.2777\n",
      "Epoch 1: train loss 0.1688, val loss 0.2782\n",
      "Epoch 2 stopped early: train loss 0.0653, val loss 0.3005\n"
     ]
    }
   ],
   "source": [
    "#Calculating the best model hyperparameters using a grid search algorithm\n",
    "\n",
    "embedding_size_list = [200,100, 50]\n",
    "learning_rate_list = [0.01,0.05, 0.1]\n",
    "drop_out_rate_list = [0.4, 0.2,0.1]\n",
    "Accuracy = []\n",
    "Fmeasure_list = []\n",
    "para_list = []\n",
    "\n",
    "for dim in embedding_size_list:\n",
    "    for lr in learning_rate_list:\n",
    "        for drop in drop_out_rate_list:\n",
    "            W = network_weights(vocab_size=len(vocab), \n",
    "                    embedding_dim=dim, \n",
    "                    hidden_dim=[], \n",
    "                    num_classes=3, \n",
    "                    init_val = 0.5)\n",
    "            \n",
    "           # We replace the dim, drop and lr for each iteration\n",
    "            W, loss_tr, dev_loss = SGD(X_tr,Y_tr,W,X_dev=X_dev, Y_dev=Y_dev,lr=lr, dropout = drop,freeze_emb=False,tolerance=0.1,epochs=100,print_progress=True)           \n",
    "            \n",
    "            # obtaining the Results and Scores with different combination\n",
    "            preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) for x,y in zip(X_test,test_class)]\n",
    "            accuracy = accuracy_score(test_class,preds_te)\n",
    "            precision = precision_score(Y_te,preds_te,average='macro')\n",
    "            Recall =recall_score(Y_te,preds_te,average='macro')\n",
    "            F_Score =f1_score(Y_te,preds_te,average='macro')\n",
    "            score = [accuracy, precision, Recall,F_Score]\n",
    "            \n",
    "            Accuracy.append(accuracy)\n",
    "            Fmeasure_list.append([accuracy, precision, Recall, F_Score])\n",
    "            para = [dim, lr, drop]\n",
    "            para_list.append(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out the best Hyperparameters\n",
    "\n",
    "We can use a grid search algorithm to find the best combination of hyperparameters for our model. The hyperparameters that are being varied in this grid search are the learning rate, embedding size, and dropout rate.\n",
    "\n",
    "For each combination of hyperparameters, the function trains the model using the SGD function and computes the accuracy, precision, recall, and F1-score on a test dataset. The results for each combination of hyperparameters are stored in test_score, all_list, and hyper_list.\n",
    "\n",
    "The test_score list keeps track of the accuracy scores for each iteration, while all_list stores the scores for all four evaluation metrics. The hyper_list keeps track of the hyperparameters for each iteration.\n",
    "\n",
    "By performing a grid search over the hyperparameters in this way, we can determine the best combination of hyperparameters for our model, which can improve its performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Hyperparameters and the F1-score on the test set are:\n",
      "embedding_dim: 100  lr: 0.01  drop_rate: 0.4  Accuracy: 0.8598442714126807 F measures: A/P/R/F [0.8598442714126807, 0.8612231315292967, 0.859817911557042, 0.8598021609989694]\n"
     ]
    }
   ],
   "source": [
    "#Computing the result for the combination of best hyperparameters\n",
    "\n",
    "best_result = max(Accuracy)\n",
    "best_id = Accuracy.index(best_result)\n",
    "best_para = para_list[best_id]\n",
    "best_S = Fmeasure_list[best_id]\n",
    "print(\"The best Hyperparameters and the F1-score on the test set are:\")\n",
    "print(\"embedding_dim:\", best_para[0], \" lr:\", best_para[1],\" drop_rate:\" ,best_para[2] ,\" Accuracy:\", best_result, \"F measures: A/P/R/F\",best_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameters selection\n",
    "\n",
    "As per the results obtained after the grid search algorithm I observe the following best hyperparameters-\n",
    "\n",
    "Embedding_dim = 100\n",
    "learning rate = 0.01\n",
    "dropout_rate = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained Embeddings\n",
    "\n",
    "Now re-train the network using GloVe pre-trained embeddings. You need to modify the `backward_pass` function above to stop computing gradients and updating weights of the embedding matrix.\n",
    "\n",
    "Use the function below to obtain the embedding martix for your vocabulary. Generally, that should work without any problem. If you get errors, you can modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:32.020697Z",
     "start_time": "2020-04-02T14:27:32.015733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, word2id, emb_size=300):\n",
    "    \n",
    "    w_emb = np.zeros((len(word2id), emb_size))\n",
    "    \n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split()[0]\n",
    "                     \n",
    "                if word in vocab:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[word2id[word]] +=emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:28:54.548613Z",
     "start_time": "2020-04-02T14:27:32.780248Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_glove = get_glove_embeddings(\"glove.840B.300d.zip\",\"glove.840B.300d.txt\",wordtovocab_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the weights of your network using the `network_weights` function. Second, replace the weigths of the embedding matrix with `w_glove`. Finally, train the network by freezing the embedding weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (2000, 300)\n",
      "Shape W1 (300, 3)\n",
      "Epoch 1: train loss 0.5543, val loss 0.5860\n",
      "Epoch 2: train loss 0.4484, val loss 0.4626\n",
      "Epoch 3: train loss 0.4054, val loss 0.4030\n",
      "Epoch 4: train loss 0.3906, val loss 0.3843\n",
      "Epoch 5: train loss 0.3747, val loss 0.3439\n",
      "Epoch 6: train loss 0.3619, val loss 0.3333\n",
      "Epoch 7: train loss 0.3539, val loss 0.3138\n",
      "Epoch 8: train loss 0.3477, val loss 0.3040\n",
      "Epoch 9: train loss 0.3428, val loss 0.3035\n",
      "Epoch 10 stopped early: train loss 0.3447, val loss 0.3107\n"
     ]
    }
   ],
   "source": [
    "# Initialise the weights of your network \n",
    "W = network_weights(vocab_size=len(vocab),embedding_dim=300,hidden_dim=[], num_classes=3, init_val = 0.1)\n",
    "\n",
    "#Replace the weigths of the embedding matrix with w_glove\n",
    "W[0] = w_glove\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print('Shape W'+str(i), W[i].shape)\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(X_tr,Y_tr,W,X_dev=X_dev, Y_dev=Y_dev,lr=0.01,\n",
    "                                       dropout = drop,freeze_emb = True,tolerance=0.001,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the Loss history \n",
    "\n",
    "plt.plot(loss_tr, label='Training loss')\n",
    "plt.plot(dev_loss, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss History')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:12:00.815184Z",
     "start_time": "2020-04-02T15:12:00.812563Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8687430478309233\n",
      "Precision: 0.8746289515573427\n",
      "Recall: 0.8687551096246748\n",
      "F1-Score: 0.8698793873203966\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) \n",
    "            for x,y in zip(X_te,Y_te)]\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.4184, val loss 0.4682\n",
      "Epoch 2: train loss 0.3301, val loss 0.3108\n",
      "Epoch 3: train loss 0.3298, val loss 0.2959\n",
      "Epoch 4 stopped early: train loss 0.2943, val loss 0.3180\n",
      "Epoch 1: train loss 0.2508, val loss 0.3147\n",
      "Epoch 2: train loss 0.2149, val loss 0.2704\n",
      "Epoch 3: train loss 0.1896, val loss 0.2642\n",
      "Epoch 4 stopped early: train loss 0.1634, val loss 0.2803\n",
      "Epoch 1: train loss 0.2116, val loss 0.3104\n",
      "Epoch 2: train loss 0.1708, val loss 0.2684\n",
      "Epoch 3: train loss 0.1490, val loss 0.2552\n",
      "Epoch 4 stopped early: train loss 0.1322, val loss 0.2631\n",
      "Epoch 1: train loss 0.2255, val loss 0.4152\n",
      "Epoch 2 stopped early: train loss 0.1834, val loss 0.6140\n",
      "Epoch 1: train loss 0.0650, val loss 0.3223\n",
      "Epoch 2 stopped early: train loss 0.0440, val loss 0.3596\n",
      "Epoch 1: train loss 0.0445, val loss 0.3374\n",
      "Epoch 2 stopped early: train loss 0.0298, val loss 0.3423\n",
      "Epoch 1: train loss 0.2513, val loss 1.0762\n",
      "Epoch 2 stopped early: train loss 0.1421, val loss 1.2062\n",
      "Epoch 1: train loss 0.0299, val loss 0.3997\n",
      "Epoch 2 stopped early: train loss 0.0183, val loss 0.6191\n",
      "Epoch 1: train loss 0.0145, val loss 0.4075\n",
      "Epoch 2 stopped early: train loss 0.0115, val loss 0.4598\n"
     ]
    }
   ],
   "source": [
    "#Calculating the best model hyperparameters using a grid search algorithm\n",
    "\n",
    "learning_rate_list = [0.01, 0.05, 0.1]\n",
    "drop_out_rate_list = [0.4, 0.2, 0.1]\n",
    "Accuracy = []\n",
    "Fmeasure_list = []\n",
    "para_list = []\n",
    "\n",
    "for lr in learning_rate_list:\n",
    "    for drop in drop_out_rate_list:\n",
    "        # Initialise the weights of your network \n",
    "        W = network_weights(vocab_size=len(vocab),embedding_dim=300,hidden_dim=[], num_classes=3, init_val = 0.1)\n",
    "            #Replace the weigths of the embedding matrix with w_glove\n",
    "        W[0] = w_glove\n",
    "        W, loss_tr, dev_loss = SGD(X_tr,Y_tr,W,X_dev=X_dev, Y_dev=Y_dev,lr=lr, dropout = drop,freeze_emb=False,tolerance=0.01,epochs=100,print_progress=True)\n",
    "   \n",
    "        preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) for x,y in zip(X_tr,train_class)]\n",
    "        tr_f1 = f1_score(train_class,preds_te,average='macro')\n",
    "            \n",
    "        preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) for x,y in zip(X_test,test_class)]\n",
    "        accuracy = accuracy_score(test_class,preds_te)\n",
    "        precision = precision_score(Y_te,preds_te,average='macro')\n",
    "        Recall =recall_score(Y_te,preds_te,average='macro')\n",
    "        F_Score =f1_score(Y_te,preds_te,average='macro')\n",
    "        score = [accuracy, precision, Recall,F_Score]\n",
    "            \n",
    "        Accuracy.append(accuracy)\n",
    "        Fmeasure_list.append([accuracy, precision, Recall, F_Score])\n",
    "        para = [dim, lr, drop]\n",
    "        para_list.append(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Hyperparameters and the F1-score on the test set are:\n",
      "embedding_dim: 50  lr: 0.01  drop_rate: 0.4  Accuracy: 0.8776418242491657 F measures: A/P/R/F [0.8776418242491657, 0.8778055578988848, 0.8776031215161652, 0.8769721541366806]\n"
     ]
    }
   ],
   "source": [
    "#Computing the result for the combination of best hyperparameters\n",
    "\n",
    "best_result = max(Accuracy)\n",
    "best_id = Accuracy.index(best_result)\n",
    "best_para = para_list[best_id]\n",
    "best_S = Fmeasure_list[best_id]\n",
    "print(\"The best Hyperparameters and the F1-score on the test set are:\")\n",
    "print(\"embedding_dim:\", best_para[0], \" lr:\", best_para[1],\" drop_rate:\" ,best_para[2] ,\" Accuracy:\", best_result, \"F measures: A/P/R/F\",best_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to support deeper architectures \n",
    "\n",
    "Extend the network to support back-propagation for more hidden layers. You need to modify the `backward_pass` function above to compute gradients and update the weights between intermediate hidden layers. Finally, train and evaluate a network with a deeper architecture. Do deeper architectures increase performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.01, freeze_emb=False):\n",
    "    \n",
    "    # Initialize gradients\n",
    "    dW = [np.zeros_like(w) for w in W]\n",
    "    \n",
    "    # Compute error at output layer\n",
    "    error = out_vals['y']\n",
    "    error[y] -= 1\n",
    "    \n",
    "    # Backpropagate error through output layer\n",
    "    dW[-1] = np.outer(out_vals['a'][-1], error)\n",
    "    delta = error\n",
    "    \n",
    "    # Backpropagate error through hidden layers\n",
    "    for i in range(len(W) - 2, 0, -1):\n",
    "        # Retrieve dropout mask for current layer\n",
    "        dropout = out_vals['dropout_vecs'][i-1]\n",
    "        # Compute delta for current layer\n",
    "        delta = np.dot(W[i+1], delta) * relu_derivative(out_vals['h'][i])\n",
    "        delta *= dropout\n",
    "        # Compute gradients for current layer\n",
    "        dW[i] = np.outer(out_vals['a'][i], delta)\n",
    "    \n",
    "    # Backpropagate error through input layer\n",
    "    delta = np.dot(W[1], delta) * relu_derivative(out_vals['h'][0])\n",
    "    dW[0] = np.zeros_like(W[0])\n",
    "    for i in x:\n",
    "        dW[0][i,:] += delta\n",
    "    if freeze_emb:\n",
    "        dW[0] = np.zeros_like(W[0])\n",
    "    \n",
    "    # Update weights\n",
    "    for i in range(len(W)):\n",
    "        W[i] -= lr * dW[i]\n",
    "    \n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:58:51.764619Z",
     "start_time": "2020-04-02T14:58:47.483690Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W0 (2000, 300)\n",
      "Shape of W1 (300, 300)\n",
      "Shape of W2 (300, 3)\n",
      "Epoch 1: train loss 0.5706, val loss 0.6889\n",
      "Epoch 2: train loss 0.3294, val loss 0.4885\n",
      "Epoch 3: train loss 0.2165, val loss 0.3952\n",
      "Epoch 4: train loss 0.1633, val loss 0.3527\n",
      "Epoch 5: train loss 0.1363, val loss 0.3286\n",
      "Epoch 6: train loss 0.1222, val loss 0.3232\n",
      "Epoch 7: train loss 0.1155, val loss 0.3157\n",
      "Epoch 8: train loss 0.1123, val loss 0.3149\n",
      "Epoch 9: train loss 0.1139, val loss 0.3053\n",
      "Epoch 10 stopped early: train loss 0.1111, val loss 0.3110\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(vocab), embedding_dim=300, hidden_dim=[300], num_classes=3, init_val=0.1)\n",
    "W[0] = w_glove\n",
    "for i in range(len(W)):\n",
    "    print('Shape of W' + str(i), W[i].shape)\n",
    "\n",
    "W, loss_tr, dev_loss = SGD(X_tr, Y_tr, W, X_dev=X_dev, Y_dev=Y_dev, lr=0.001, dropout=0.2, freeze_emb=True,\n",
    "                            print_progress=True, tolerance=0.00001, epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the Loss history \n",
    "\n",
    "plt.plot(loss_tr, label='Training loss')\n",
    "plt.plot(dev_loss, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss History')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:11:51.994986Z",
     "start_time": "2020-04-02T15:11:51.992563Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8264738598442715\n",
      "Precision: 0.8272272252030403\n",
      "Recall: 0.8264437012263098\n",
      "F1-Score: 0.8257476264906649\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) \n",
    "            for x,y in zip(X_te,Y_te)]\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_te,preds_te))\n",
    "print('Precision:', precision_score(Y_te,preds_te,average='macro'))\n",
    "print('Recall:', recall_score(Y_te,preds_te,average='macro'))\n",
    "print('F1-Score:', f1_score(Y_te,preds_te,average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.4560, val loss 0.5867\n",
      "Epoch 2: train loss 0.2481, val loss 0.4195\n",
      "Epoch 3: train loss 0.1705, val loss 0.3628\n",
      "Epoch 4: train loss 0.1329, val loss 0.3430\n",
      "Epoch 5: train loss 0.1118, val loss 0.3333\n",
      "Epoch 6: train loss 0.0984, val loss 0.3287\n",
      "Epoch 7: train loss 0.0895, val loss 0.3277\n",
      "Epoch 8 stopped early: train loss 0.0831, val loss 0.3288\n",
      "Epoch 1: train loss 0.5610, val loss 0.6770\n",
      "Epoch 2: train loss 0.3045, val loss 0.4700\n",
      "Epoch 3: train loss 0.1866, val loss 0.3822\n",
      "Epoch 4: train loss 0.1336, val loss 0.3446\n",
      "Epoch 5: train loss 0.1023, val loss 0.3299\n",
      "Epoch 6: train loss 0.0853, val loss 0.3247\n",
      "Epoch 7 stopped early: train loss 0.0738, val loss 0.3266\n",
      "Epoch 1: train loss 0.5988, val loss 0.6922\n",
      "Epoch 2: train loss 0.3268, val loss 0.4662\n",
      "Epoch 3: train loss 0.1898, val loss 0.3499\n",
      "Epoch 4: train loss 0.1223, val loss 0.3007\n",
      "Epoch 5: train loss 0.0897, val loss 0.2817\n",
      "Epoch 6 stopped early: train loss 0.0739, val loss 0.2853\n",
      "Epoch 1: train loss 0.0781, val loss 0.3003\n",
      "Epoch 2 stopped early: train loss 0.0599, val loss 0.3190\n",
      "Epoch 1: train loss 0.0612, val loss 0.2888\n",
      "Epoch 2 stopped early: train loss 0.0805, val loss 0.3217\n",
      "Epoch 1: train loss 0.0716, val loss 0.3018\n",
      "Epoch 2 stopped early: train loss 0.1130, val loss 0.3376\n",
      "Epoch 1: train loss 0.0564, val loss 0.3212\n",
      "Epoch 2 stopped early: train loss 0.0255, val loss 0.3283\n",
      "Epoch 1: train loss 0.1234, val loss 0.3993\n",
      "Epoch 2 stopped early: train loss 0.0658, val loss 0.4043\n",
      "Epoch 1: train loss 0.0876, val loss 0.6983\n",
      "Epoch 2 stopped early: train loss 0.0594, val loss 0.7530\n"
     ]
    }
   ],
   "source": [
    "#Calculating the best model hyperparameters using a grid search algorithm and using all possible combinations of the Hyperparameters\n",
    "\n",
    "learning_rate_list = [0.001,0.01,0.05]\n",
    "drop_out_rate_list = [0.01, 0.2, 0.4 ]\n",
    "Accuracy = []\n",
    "Fmeasure_list = []\n",
    "para_list = []\n",
    "\n",
    "for lr in learning_rate_list:\n",
    "    for drop in drop_out_rate_list:\n",
    "        W = network_weights(vocab_size=len(vocab),embedding_dim=300,hidden_dim=[300],num_classes=3, init_val = 0.1)\n",
    "            #Replace the weigths of the embedding matrix with w_glove\n",
    "        W[0] = w_glove\n",
    "        W, loss_tr, dev_loss = SGD(X_tr,Y_tr,W,X_dev=X_dev, Y_dev=Y_dev,lr=lr, \n",
    "                                   dropout = drop,freeze_emb=False,tolerance=0.1,epochs=100,print_progress=True)    \n",
    "        # Results and Scores\n",
    "        preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) for x,y in zip(X_test,test_class)]\n",
    "        accuracy = accuracy_score(test_class,preds_te)\n",
    "        precision = precision_score(Y_te,preds_te,average='macro')\n",
    "        Recall =recall_score(Y_te,preds_te,average='macro')\n",
    "        F_Score =f1_score(Y_te,preds_te,average='macro')\n",
    "        score = [accuracy, precision, Recall,F_Score]\n",
    "            \n",
    "        Accuracy.append(accuracy)\n",
    "        Fmeasure_list.append([accuracy, precision, Recall, F_Score])\n",
    "        para = [dim, lr, drop]\n",
    "        para_list.append(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Hyperparameters and the F1-score on the test set are:\n",
      "embedding_dim: 50  lr: 0.01  drop_rate: 0.4  Accuracy: 0.8654060066740823 F measures: A/P/R/F [0.8654060066740823, 0.868037103662937, 0.8653920475659606, 0.8649193899657908]\n"
     ]
    }
   ],
   "source": [
    "#Computing the result for the combination of best hyperparameters\n",
    "\n",
    "best_result = max(Accuracy)\n",
    "best_id = Accuracy.index(best_result)\n",
    "best_para = para_list[best_id]\n",
    "best_S = Fmeasure_list[best_id]\n",
    "print(\"The best Hyperparameters and the F1-score on the test set are:\")\n",
    "print(\"embedding_dim:\", best_para[0], \" lr:\", best_para[1],\" drop_rate:\" ,best_para[2] ,\" Accuracy:\", best_result, \"F measures: A/P/R/F\",best_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "Add your final results here:\n",
    "\n",
    "| Model | Precision  | Recall  | F1-Score  | Accuracy\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| Average Embedding  |0.8612231315292967   |0.859817911557042   |0.8598021609989694   |0.8598442714126807   |\n",
    "| Average Embedding (Pre-trained)  |0.8778055578988848   |0.8776031215161652   |0.8769721541366806   |0.8776418242491657   |\n",
    "| Average Embedding (Pre-trained) + X hidden layers    |0.868037103662937  |0.8653920475659606   |0.8649193899657908   |0.8654060066740823   |\n",
    "\n",
    "\n",
    "Please discuss why your best performing model is better than the rest and provide a bried error analaysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "The impact of deeper architectures on performance depends on the specific model and data.\n",
    "\n",
    "Deeper architectures often allow the network to learn more intricate and abstract representations of the input data, which may increase performance. Deeper designs, however, can raise the chance of overfitting, particularly if the data is sparse or chaotic.\n",
    "\n",
    "I get better results with my Pre-trained model - This signfies that my  pre-trained embeddings are better suited to the task at hand than the embeddings learned during training.\n",
    "\n",
    "Maybe with more model training my Hidden Layers model can obtain better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
